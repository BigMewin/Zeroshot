{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0fec278",
   "metadata": {},
   "source": [
    "This is MLP method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49eabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd84161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2287692/2575041874.py:3: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "# 104 gene for training\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "# 26 gene for testing\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "\n",
    "# filter out compound        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "\n",
    "# To meet the mAP evaluation Plate\n",
    "# 2x2 : Time_delay x Cell_line\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "\n",
    "\n",
    "\n",
    "# SEEN test\n",
    "df_test1=pd.concat([train_part_total,dftrain2,df_negcon]) \n",
    "\n",
    "# Train \n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) \n",
    "\n",
    "### Unseen test\n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=pd.concat([df_test2,df_negcon,dftrain2])\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "testX=df_test1.drop('Metadata_gene', axis = 1)\n",
    "y_test = df_test1['Metadata_gene']\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9be759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a746e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.64302409\n",
      "Iteration 2, loss = 3.80418899\n",
      "Iteration 3, loss = 3.15556815\n",
      "Iteration 4, loss = 2.59989785\n",
      "Iteration 5, loss = 2.18585988\n",
      "Iteration 6, loss = 1.75207273\n",
      "Iteration 7, loss = 1.41129850\n",
      "Iteration 8, loss = 1.08236015\n",
      "Iteration 9, loss = 0.81836616\n",
      "Iteration 10, loss = 0.58544331\n",
      "Iteration 11, loss = 0.44566242\n",
      "Iteration 12, loss = 0.33872538\n",
      "Iteration 13, loss = 0.22291450\n",
      "Iteration 14, loss = 0.18422597\n",
      "Iteration 15, loss = 0.12365416\n",
      "Iteration 16, loss = 0.12011507\n",
      "Iteration 17, loss = 0.13772186\n",
      "Iteration 18, loss = 0.22552734\n",
      "Iteration 19, loss = 0.30222012\n",
      "Iteration 20, loss = 0.17891568\n",
      "Iteration 21, loss = 0.13227664\n",
      "Iteration 22, loss = 0.08759237\n",
      "Iteration 23, loss = 0.09837746\n",
      "Iteration 24, loss = 0.05809768\n",
      "Iteration 25, loss = 0.04464473\n",
      "Iteration 26, loss = 0.02680975\n",
      "Iteration 27, loss = 0.01565480\n",
      "Iteration 28, loss = 0.01247090\n",
      "Iteration 29, loss = 0.03757407\n",
      "Iteration 30, loss = 0.05872848\n",
      "Iteration 31, loss = 0.05724108\n",
      "Iteration 32, loss = 0.10642590\n",
      "Iteration 33, loss = 0.08282629\n",
      "Iteration 34, loss = 0.07267666\n",
      "Iteration 35, loss = 0.06524586\n",
      "Iteration 36, loss = 0.03900888\n",
      "Iteration 37, loss = 0.01730385\n",
      "Iteration 38, loss = 0.02143230\n",
      "Iteration 39, loss = 0.02096699\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(1280, 630, 300, 176), max_iter=300, tol=1e-08,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(1280, 630, 300, 176), max_iter=300, tol=1e-08,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(1280, 630, 300, 176), max_iter=300, tol=1e-08,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "# Scale the training and test sets using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "trainX_scaled = scaler.fit_transform(trainX)\n",
    "testX_scaled = scaler.transform(testX)\n",
    "# Train the MLPClassifier model\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(1280, 630, 300, 176),\n",
    "                        max_iter=300, activation='relu', verbose=True, tol=1e-8,\n",
    "                        solver='adam')\n",
    "mlp_clf.fit(trainX_scaled, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d799b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc591ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Metadata_broad_sample</th>\n",
       "      <th>Metadata_solvent</th>\n",
       "      <th>Metadata_experiment_type</th>\n",
       "      <th>Metadata_Plate</th>\n",
       "      <th>Metadata_Well</th>\n",
       "      <th>Metadata_InChIKey</th>\n",
       "      <th>Metadata_pert_iname</th>\n",
       "      <th>Metadata_pubchem_cid</th>\n",
       "      <th>Metadata_gene</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Metadata_experiment_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12670</th>\n",
       "      <td>0</td>\n",
       "      <td>BRD-A86665761-001-01-1</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00116991</td>\n",
       "      <td>A01</td>\n",
       "      <td>TZDUHAJSIBHXDL-UHFFFAOYSA-N</td>\n",
       "      <td>gabapentin-enacarbil</td>\n",
       "      <td>9883900.0</td>\n",
       "      <td>CACNB4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146942</td>\n",
       "      <td>3.784249</td>\n",
       "      <td>1.477168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12672</th>\n",
       "      <td>2</td>\n",
       "      <td>BRD-A22032524-074-09-9</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00116991</td>\n",
       "      <td>A03</td>\n",
       "      <td>HTIQEAQVCYTUBX-UHFFFAOYSA-N</td>\n",
       "      <td>amlodipine</td>\n",
       "      <td>2162.0</td>\n",
       "      <td>CACNA2D3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.273576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12673</th>\n",
       "      <td>3</td>\n",
       "      <td>BRD-A01078468-001-14-8</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00116991</td>\n",
       "      <td>A04</td>\n",
       "      <td>PBBGSZCBWVPOOL-UHFFFAOYSA-N</td>\n",
       "      <td>hexestrol</td>\n",
       "      <td>3606.0</td>\n",
       "      <td>AKR1C1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.540692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12675</th>\n",
       "      <td>5</td>\n",
       "      <td>BRD-K36574127-001-01-3</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00116991</td>\n",
       "      <td>A06</td>\n",
       "      <td>NYNZQNWKBKUAII-KBXCAEBGSA-N</td>\n",
       "      <td>LOXO-101</td>\n",
       "      <td>46189000.0</td>\n",
       "      <td>NTRK1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12676</th>\n",
       "      <td>6</td>\n",
       "      <td>BRD-K74913225-001-14-0</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00116991</td>\n",
       "      <td>A07</td>\n",
       "      <td>HCRKCZRJWPKOAR-JTQLQIEISA-N</td>\n",
       "      <td>brinzolamide</td>\n",
       "      <td>68844.0</td>\n",
       "      <td>CA5A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.158628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19447</th>\n",
       "      <td>333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00117024</td>\n",
       "      <td>N22</td>\n",
       "      <td>IAZDPXIOMUYVGZ-UHFFFAOYSA-N</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>679.0</td>\n",
       "      <td>empty</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19456</th>\n",
       "      <td>342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00117024</td>\n",
       "      <td>O07</td>\n",
       "      <td>IAZDPXIOMUYVGZ-UHFFFAOYSA-N</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>679.0</td>\n",
       "      <td>empty</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19467</th>\n",
       "      <td>353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00117024</td>\n",
       "      <td>O18</td>\n",
       "      <td>IAZDPXIOMUYVGZ-UHFFFAOYSA-N</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>679.0</td>\n",
       "      <td>empty</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19478</th>\n",
       "      <td>364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00117024</td>\n",
       "      <td>P05</td>\n",
       "      <td>IAZDPXIOMUYVGZ-UHFFFAOYSA-N</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>679.0</td>\n",
       "      <td>empty</td>\n",
       "      <td>...</td>\n",
       "      <td>15.975267</td>\n",
       "      <td>1.736075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19483</th>\n",
       "      <td>369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>Compound</td>\n",
       "      <td>BR00117024</td>\n",
       "      <td>P10</td>\n",
       "      <td>IAZDPXIOMUYVGZ-UHFFFAOYSA-N</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>679.0</td>\n",
       "      <td>empty</td>\n",
       "      <td>...</td>\n",
       "      <td>4.620309</td>\n",
       "      <td>2.119358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Compound</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11104 rows × 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   Metadata_broad_sample Metadata_solvent  \\\n",
       "12670           0  BRD-A86665761-001-01-1             DMSO   \n",
       "12672           2  BRD-A22032524-074-09-9             DMSO   \n",
       "12673           3  BRD-A01078468-001-14-8             DMSO   \n",
       "12675           5  BRD-K36574127-001-01-3             DMSO   \n",
       "12676           6  BRD-K74913225-001-14-0             DMSO   \n",
       "...           ...                     ...              ...   \n",
       "19447         333                     NaN             DMSO   \n",
       "19456         342                     NaN             DMSO   \n",
       "19467         353                     NaN             DMSO   \n",
       "19478         364                     NaN             DMSO   \n",
       "19483         369                     NaN             DMSO   \n",
       "\n",
       "       Metadata_experiment_type Metadata_Plate Metadata_Well  \\\n",
       "12670                  Compound     BR00116991           A01   \n",
       "12672                  Compound     BR00116991           A03   \n",
       "12673                  Compound     BR00116991           A04   \n",
       "12675                  Compound     BR00116991           A06   \n",
       "12676                  Compound     BR00116991           A07   \n",
       "...                         ...            ...           ...   \n",
       "19447                  Compound     BR00117024           N22   \n",
       "19456                  Compound     BR00117024           O07   \n",
       "19467                  Compound     BR00117024           O18   \n",
       "19478                  Compound     BR00117024           P05   \n",
       "19483                  Compound     BR00117024           P10   \n",
       "\n",
       "                 Metadata_InChIKey   Metadata_pert_iname  \\\n",
       "12670  TZDUHAJSIBHXDL-UHFFFAOYSA-N  gabapentin-enacarbil   \n",
       "12672  HTIQEAQVCYTUBX-UHFFFAOYSA-N            amlodipine   \n",
       "12673  PBBGSZCBWVPOOL-UHFFFAOYSA-N             hexestrol   \n",
       "12675  NYNZQNWKBKUAII-KBXCAEBGSA-N              LOXO-101   \n",
       "12676  HCRKCZRJWPKOAR-JTQLQIEISA-N          brinzolamide   \n",
       "...                            ...                   ...   \n",
       "19447  IAZDPXIOMUYVGZ-UHFFFAOYSA-N                  DMSO   \n",
       "19456  IAZDPXIOMUYVGZ-UHFFFAOYSA-N                  DMSO   \n",
       "19467  IAZDPXIOMUYVGZ-UHFFFAOYSA-N                  DMSO   \n",
       "19478  IAZDPXIOMUYVGZ-UHFFFAOYSA-N                  DMSO   \n",
       "19483  IAZDPXIOMUYVGZ-UHFFFAOYSA-N                  DMSO   \n",
       "\n",
       "       Metadata_pubchem_cid Metadata_gene  ...        291       292       293  \\\n",
       "12670             9883900.0        CACNB4  ...   0.146942  3.784249  1.477168   \n",
       "12672                2162.0      CACNA2D3  ...   0.000000  2.273576  0.000000   \n",
       "12673                3606.0        AKR1C1  ...   0.000000  3.540692  0.000000   \n",
       "12675            46189000.0         NTRK1  ...   0.000000  0.000000  0.000000   \n",
       "12676               68844.0          CA5A  ...   0.000000  1.158628  0.000000   \n",
       "...                     ...           ...  ...        ...       ...       ...   \n",
       "19447                 679.0         empty  ...   0.135891  0.000000  0.000000   \n",
       "19456                 679.0         empty  ...   0.000000  0.000000  0.000000   \n",
       "19467                 679.0         empty  ...   0.000000  0.000000  0.000000   \n",
       "19478                 679.0         empty  ...  15.975267  1.736075  0.000000   \n",
       "19483                 679.0         empty  ...   4.620309  2.119358  0.000000   \n",
       "\n",
       "       294  295  296  297  298  299  Metadata_experiment_type  \n",
       "12670  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "12672  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "12673  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "12675  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "12676  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "...    ...  ...  ...  ...  ...  ...                       ...  \n",
       "19447  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "19456  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "19467  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "19478  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "19483  0.0  0.0  0.0  0.0  0.0  0.0                  Compound  \n",
       "\n",
       "[11104 rows x 316 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test1.iloc[:,df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test1.iloc[:,0:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test1['Metadata_experiment_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a299c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new.to_csv('MLP_seen_new.csv')\n",
    "df_test1.to_csv('MLP_seen_old.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ce8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f124db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test2.iloc[:,df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test2.iloc[:,0:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test2['Metadata_experiment_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0261db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new.to_csv('MLP_unseen_new.csv')\n",
    "df_test2.to_csv('MLP_unseen_old.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9e123",
   "metadata": {},
   "source": [
    "This is SLPP Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a44fa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2287692/3427723339.py:3: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "\n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "\n",
    "# To meet the mAP evaluation Plate\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "\n",
    "\n",
    "\n",
    "# SEEN\n",
    "\n",
    "\n",
    "df_test1=pd.concat([train_part_total,dftrain2,df_negcon])  ####其中一种验证方法！！！\n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) ###训练\n",
    "### Unseen\n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=pd.concat([df_test2,df_negcon,dftrain2])\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "y_test=df_test1['Metadata_gene']\n",
    "testX=df_test1.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec1cc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX\n",
    "y_train = y_train.reset_index()\n",
    "y_train=y_train['Metadata_gene'].tolist()\n",
    "n_components=300\n",
    "sigma=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba04a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Standardize the data\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    # Compute the pairwise distance matrix\n",
    "    D = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            D[i, j] = np.linalg.norm(X_train[i] - X_train[j])\n",
    "\n",
    "    # Compute the adjacency graph using a Gaussian kernel\n",
    "    W = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            W[i, j] = np.exp(-D[i, j] ** 2 / (2 * sigma ** 2)) * (y_train[i] == y_train[j])\n",
    "\n",
    "    # Compute the degree matrix\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "\n",
    "    # Compute the Laplacian matrix\n",
    "    L = D - W\n",
    "\n",
    "    # Compute the eigenvectors\n",
    "    eigvals, eigvecs = np.linalg.eig(np.dot(np.dot(X_train.T, L), X_train))\n",
    "\n",
    "    # Sort the eigenvectors by eigenvalues in descending order\n",
    "    sort_indices = np.argsort(eigvals)[::-1]\n",
    "    eigvecs = eigvecs[:, sort_indices]\n",
    "\n",
    "    # Select the first n_components eigenvectors\n",
    "    eigvecs = eigvecs[:, :n_components]\n",
    "    eigvecs=np.real(eigvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d8d4be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## transformed seen_test_data\n",
    "transformed_data = np.dot(testX, eigvecs)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX1=df_test1.iloc[:,:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX1=testX1.reset_index()\n",
    "dfnew =pd.concat([testX1,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7629a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.iloc[:,1:].to_csv('SLPP_seen_new.csv')          # After feature transformation\n",
    "df_test1.to_csv('SLPP_seen_old.csv')                  # Before feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e99a9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_test pre-process Unseen test data\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "y_test=df_test2['Metadata_gene']\n",
    "testX=df_test2.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)\n",
    "X_train = trainX\n",
    "y_train = y_train.reset_index()\n",
    "y_train=y_train['Metadata_gene'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "164b513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformed Unseen test Data\n",
    "transformed_data = np.dot(testX, eigvecs)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX2=df_test2.iloc[:,:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX2=testX2.reset_index()\n",
    "dfnew =pd.concat([testX2,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f183f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.iloc[:,1:].to_csv('SLPP_unseen_new.csv')\n",
    "df_test1.to_csv('SLPP_unseen_old.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77986e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853c358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71820f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d260e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
