{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0fec278",
   "metadata": {},
   "source": [
    "This is MLP method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49eabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd84161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2407021/2695503367.py:3: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "# 104 gene for training\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "# 26 gene for testing\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "\n",
    "# filter out compound        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "\n",
    "# To meet the mAP evaluation Plate\n",
    "# 2x2 : Time_delay x Cell_line\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "\n",
    "\n",
    "\n",
    "# SEEN test\n",
    "df_test1=pd.concat([gene_seen,dftrain2,df_negcon]) \n",
    "\n",
    "# Train \n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) \n",
    "\n",
    "### Unseen test\n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=pd.concat([df_test2,df_negcon,dftrain2])\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "testX=df_test1.drop('Metadata_gene', axis = 1)\n",
    "y_test = df_test1['Metadata_gene']\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1d85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5feffb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de4dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a41ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a746e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.64535265\n",
      "Iteration 2, loss = 3.78780778\n",
      "Iteration 3, loss = 3.14816004\n",
      "Iteration 4, loss = 2.62909391\n",
      "Iteration 5, loss = 2.13012001\n",
      "Iteration 6, loss = 1.71403807\n",
      "Iteration 7, loss = 1.35575710\n",
      "Iteration 8, loss = 1.05045053\n",
      "Iteration 9, loss = 0.78307966\n",
      "Iteration 10, loss = 0.59474653\n",
      "Iteration 11, loss = 0.41090321\n",
      "Iteration 12, loss = 0.26751654\n",
      "Iteration 13, loss = 0.22328245\n",
      "Iteration 14, loss = 0.14752930\n",
      "Iteration 15, loss = 0.12864058\n",
      "Iteration 16, loss = 0.12942542\n",
      "Iteration 17, loss = 0.11701284\n",
      "Iteration 18, loss = 0.11734535\n",
      "Iteration 19, loss = 0.14349838\n",
      "Iteration 20, loss = 0.11417991\n",
      "Iteration 21, loss = 0.07507294\n",
      "Iteration 22, loss = 0.09294903\n",
      "Iteration 23, loss = 0.15965702\n",
      "Iteration 24, loss = 0.08929189\n",
      "Iteration 25, loss = 0.06252906\n",
      "Iteration 26, loss = 0.04505317\n",
      "Iteration 27, loss = 0.01729860\n",
      "Iteration 28, loss = 0.00802399\n",
      "Iteration 29, loss = 0.00599723\n",
      "Iteration 30, loss = 0.00531791\n",
      "Iteration 31, loss = 0.00954631\n",
      "Iteration 32, loss = 0.01084729\n",
      "Iteration 33, loss = 0.00573670\n",
      "Iteration 34, loss = 0.00252540\n",
      "Iteration 35, loss = 0.00198079\n",
      "Iteration 36, loss = 0.00186451\n",
      "Iteration 37, loss = 0.00180084\n",
      "Iteration 38, loss = 0.00175391\n",
      "Iteration 39, loss = 0.00171594\n",
      "Iteration 40, loss = 0.00168439\n",
      "Iteration 41, loss = 0.00165735\n",
      "Iteration 42, loss = 0.00163335\n",
      "Iteration 43, loss = 0.00161215\n",
      "Iteration 44, loss = 0.00159332\n",
      "Iteration 45, loss = 0.00157634\n",
      "Iteration 46, loss = 0.00156061\n",
      "Iteration 47, loss = 0.00154632\n",
      "Iteration 48, loss = 0.00153356\n",
      "Iteration 49, loss = 0.00152100\n",
      "Iteration 50, loss = 0.00150998\n",
      "Iteration 51, loss = 0.00149959\n",
      "Iteration 52, loss = 0.00148981\n",
      "Iteration 53, loss = 0.00148046\n",
      "Iteration 54, loss = 0.00147180\n",
      "Iteration 55, loss = 0.00146385\n",
      "Iteration 56, loss = 0.00145614\n",
      "Iteration 57, loss = 0.00144873\n",
      "Iteration 58, loss = 0.00144196\n",
      "Iteration 59, loss = 0.00143532\n",
      "Iteration 60, loss = 0.00142904\n",
      "Iteration 61, loss = 0.00142316\n",
      "Iteration 62, loss = 0.00141758\n",
      "Iteration 63, loss = 0.00141201\n",
      "Iteration 64, loss = 0.00140670\n",
      "Iteration 65, loss = 0.00140194\n",
      "Iteration 66, loss = 0.00139700\n",
      "Iteration 67, loss = 0.00139236\n",
      "Iteration 68, loss = 0.00138794\n",
      "Iteration 69, loss = 0.00138366\n",
      "Iteration 70, loss = 0.00137960\n",
      "Iteration 71, loss = 0.00137555\n",
      "Iteration 72, loss = 0.00137166\n",
      "Iteration 73, loss = 0.00136805\n",
      "Iteration 74, loss = 0.00136437\n",
      "Iteration 75, loss = 0.00136100\n",
      "Iteration 76, loss = 0.00135762\n",
      "Iteration 77, loss = 0.00135431\n",
      "Iteration 78, loss = 0.00135109\n",
      "Iteration 79, loss = 0.00134802\n",
      "Iteration 80, loss = 0.00134504\n",
      "Iteration 81, loss = 0.00134212\n",
      "Iteration 82, loss = 0.00133936\n",
      "Iteration 83, loss = 0.00133659\n",
      "Iteration 84, loss = 0.00133396\n",
      "Iteration 85, loss = 0.00133138\n",
      "Iteration 86, loss = 0.00132891\n",
      "Iteration 87, loss = 0.00132642\n",
      "Iteration 88, loss = 0.00132394\n",
      "Iteration 89, loss = 0.00132163\n",
      "Iteration 90, loss = 0.00131936\n",
      "Iteration 91, loss = 0.00131706\n",
      "Iteration 92, loss = 0.00131481\n",
      "Iteration 93, loss = 0.00131269\n",
      "Iteration 94, loss = 0.00131064\n",
      "Iteration 95, loss = 0.00130849\n",
      "Iteration 96, loss = 0.00130654\n",
      "Iteration 97, loss = 0.00130451\n",
      "Iteration 98, loss = 0.00130254\n",
      "Iteration 99, loss = 0.00130061\n",
      "Iteration 100, loss = 0.00129885\n",
      "Iteration 101, loss = 0.00129696\n",
      "Iteration 102, loss = 0.00129515\n",
      "Iteration 103, loss = 0.00129331\n",
      "Iteration 104, loss = 0.00129158\n",
      "Iteration 105, loss = 0.00128987\n",
      "Iteration 106, loss = 0.00128813\n",
      "Iteration 107, loss = 0.00128650\n",
      "Iteration 108, loss = 0.00128478\n",
      "Iteration 109, loss = 0.00128319\n",
      "Iteration 110, loss = 0.00128155\n",
      "Iteration 111, loss = 0.00127999\n",
      "Iteration 112, loss = 0.00127845\n",
      "Iteration 113, loss = 0.00127676\n",
      "Iteration 114, loss = 0.00127520\n",
      "Iteration 115, loss = 0.00127368\n",
      "Iteration 116, loss = 0.00127219\n",
      "Iteration 117, loss = 0.00127069\n",
      "Iteration 118, loss = 0.00126915\n",
      "Iteration 119, loss = 0.00126769\n",
      "Iteration 120, loss = 0.00126614\n",
      "Iteration 121, loss = 0.00126476\n",
      "Iteration 122, loss = 0.00126348\n",
      "Iteration 123, loss = 0.00126190\n",
      "Iteration 124, loss = 0.00126053\n",
      "Iteration 125, loss = 0.00125919\n",
      "Iteration 126, loss = 0.00125792\n",
      "Iteration 127, loss = 0.00125643\n",
      "Iteration 128, loss = 0.00125511\n",
      "Iteration 129, loss = 0.00125379\n",
      "Iteration 130, loss = 0.00125239\n",
      "Iteration 131, loss = 0.00125119\n",
      "Iteration 132, loss = 0.00124979\n",
      "Iteration 133, loss = 0.00124857\n",
      "Iteration 134, loss = 0.00124732\n",
      "Iteration 135, loss = 0.00124614\n",
      "Iteration 136, loss = 0.00124482\n",
      "Iteration 137, loss = 0.00124357\n",
      "Iteration 138, loss = 0.00124235\n",
      "Iteration 139, loss = 0.00124118\n",
      "Iteration 140, loss = 0.00123995\n",
      "Iteration 141, loss = 0.00123875\n",
      "Iteration 142, loss = 0.00123774\n",
      "Iteration 143, loss = 0.00123637\n",
      "Iteration 144, loss = 0.00123526\n",
      "Iteration 145, loss = 0.00123413\n",
      "Iteration 146, loss = 0.00123286\n",
      "Iteration 147, loss = 0.00123168\n",
      "Iteration 148, loss = 0.00123060\n",
      "Iteration 149, loss = 0.00122942\n",
      "Iteration 150, loss = 0.00122820\n",
      "Iteration 151, loss = 0.00122706\n",
      "Iteration 152, loss = 0.00122589\n",
      "Iteration 153, loss = 0.00122471\n",
      "Iteration 154, loss = 0.00122358\n",
      "Iteration 155, loss = 0.00122252\n",
      "Iteration 156, loss = 0.00122133\n",
      "Iteration 157, loss = 0.00122019\n",
      "Iteration 158, loss = 0.00121905\n",
      "Iteration 159, loss = 0.00121788\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "# Scale the training and test sets using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "trainX_scaled = scaler.fit_transform(trainX)\n",
    "testX_scaled = scaler.transform(testX)\n",
    "# Train the MLPClassifier model\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(1280, 630, 300, 176),\n",
    "                        max_iter=300, activation='relu', verbose=True, tol=1e-8,\n",
    "                        solver='adam')\n",
    "mlp_clf.fit(trainX_scaled, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d799b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc591ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test1.iloc[:,df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test1.iloc[:,0:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test1['Metadata_experiment_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074a7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc382f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122a324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a299c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new.to_csv('MLP_seen_new.csv')\n",
    "df_test1.to_csv('MLP_seen_old.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ce8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f124db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test2.iloc[:,df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test2.iloc[:,0:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test2['Metadata_experiment_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new.to_csv('MLP_unseen_new.csv')\n",
    "df_test2.to_csv('MLP_unseen_old.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9e123",
   "metadata": {},
   "source": [
    "This is SLPP Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a44fa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "\n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "\n",
    "# To meet the mAP evaluation Plate\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "\n",
    "\n",
    "\n",
    "# SEEN\n",
    "\n",
    "\n",
    "df_test1=pd.concat([gene_seen,dftrain2,df_negcon])  ####其中一种验证方法！！！\n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) ###训练\n",
    "### Unseen\n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=pd.concat([df_test2,df_negcon,dftrain2])\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "y_test=df_test1['Metadata_gene']\n",
    "testX=df_test1.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e111b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4846c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e32fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1cc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX\n",
    "y_train = y_train.reset_index()\n",
    "y_train=y_train['Metadata_gene'].tolist()\n",
    "n_components=300\n",
    "sigma=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Standardize the data\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    # Compute the pairwise distance matrix\n",
    "    D = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            D[i, j] = np.linalg.norm(X_train[i] - X_train[j])\n",
    "\n",
    "    # Compute the adjacency graph using a Gaussian kernel\n",
    "    W = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            W[i, j] = np.exp(-D[i, j] ** 2 / (2 * sigma ** 2)) * (y_train[i] == y_train[j])\n",
    "\n",
    "    # Compute the degree matrix\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "\n",
    "    # Compute the Laplacian matrix\n",
    "    L = D - W\n",
    "\n",
    "    # Compute the eigenvectors\n",
    "    eigvals, eigvecs = np.linalg.eig(np.dot(np.dot(X_train.T, L), X_train))\n",
    "\n",
    "    # Sort the eigenvectors by eigenvalues in descending order\n",
    "    sort_indices = np.argsort(eigvals)[::-1]\n",
    "    eigvecs = eigvecs[:, sort_indices]\n",
    "\n",
    "    # Select the first n_components eigenvectors\n",
    "    eigvecs = eigvecs[:, :n_components]\n",
    "    eigvecs=np.real(eigvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8d4be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## transformed seen_test_data\n",
    "transformed_data = np.dot(testX, eigvecs)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX1=df_test1.iloc[:,:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX1=testX1.reset_index()\n",
    "dfnew =pd.concat([testX1,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7629a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.iloc[:,1:].to_csv('SLPP_seen_new.csv')          # After feature transformation\n",
    "df_test1.to_csv('SLPP_seen_old.csv')                  # Before feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_test pre-process Unseen test data\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "y_test=df_test2['Metadata_gene']\n",
    "testX=df_test2.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)\n",
    "X_train = trainX\n",
    "y_train = y_train.reset_index()\n",
    "y_train=y_train['Metadata_gene'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformed Unseen test Data\n",
    "transformed_data = np.dot(testX, eigvecs)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX2=df_test2.iloc[:,:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX2=testX2.reset_index()\n",
    "dfnew =pd.concat([testX2,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.iloc[:,1:].to_csv('SLPP_unseen_new.csv')\n",
    "df_test1.to_csv('SLPP_unseen_old.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77986e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853c358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71820f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d260e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
