{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0fec278",
   "metadata": {},
   "source": [
    "This is MLP method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49eabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd84161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2979650/209746574.py:3: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df['Metadata_experiment_type'].value_counts()\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "# 104 gene for training\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "# 26 gene for testing\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "\n",
    "# filter out compound        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "\n",
    "# To meet the mAP evaluation Plate\n",
    "# 2x2 : Time_delay x Cell_line\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "train_part_total_final=train_part_total_final.drop_duplicates()\n",
    "\n",
    "\n",
    "# SEEN test\n",
    "df_test1=pd.concat([gene_seen,dftrain2,df_negcon]) \n",
    "#df_test1=df_test1[df_test1['Metadata_Plate'].isin(lst)]\n",
    "# Train \n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) \n",
    "\n",
    "### Unseen test\n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=pd.concat([df_test2,df_negcon,dftrain2])\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "#df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "testX=df_test1.drop('Metadata_gene', axis = 1)\n",
    "y_test = df_test1['Metadata_gene']\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d58b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37baaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ea668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa292c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a746e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.28759913\n",
      "Iteration 2, loss = 3.24805303\n",
      "Iteration 3, loss = 2.59879187\n",
      "Iteration 4, loss = 2.12215408\n",
      "Iteration 5, loss = 1.70047074\n",
      "Iteration 6, loss = 1.34758674\n",
      "Iteration 7, loss = 1.09273666\n",
      "Iteration 8, loss = 0.81640017\n",
      "Iteration 9, loss = 0.56275672\n",
      "Iteration 10, loss = 0.46550219\n",
      "Iteration 11, loss = 0.29208065\n",
      "Iteration 12, loss = 0.23931224\n",
      "Iteration 13, loss = 0.23256273\n",
      "Iteration 14, loss = 0.17159830\n",
      "Iteration 15, loss = 0.17416509\n",
      "Iteration 16, loss = 0.09997849\n",
      "Iteration 17, loss = 0.06502833\n",
      "Iteration 18, loss = 0.04792372\n",
      "Iteration 19, loss = 0.06673045\n",
      "Iteration 20, loss = 0.03483443\n",
      "Iteration 21, loss = 0.01736771\n",
      "Iteration 22, loss = 0.01142495\n",
      "Iteration 23, loss = 0.00602086\n",
      "Iteration 24, loss = 0.00578972\n",
      "Iteration 25, loss = 0.00405457\n",
      "Iteration 26, loss = 0.00426730\n",
      "Iteration 27, loss = 0.00447060\n",
      "Iteration 28, loss = 0.00459013\n",
      "Iteration 29, loss = 0.00447451\n",
      "Iteration 30, loss = 0.00322939\n",
      "Iteration 31, loss = 0.00282946\n",
      "Iteration 32, loss = 0.00267698\n",
      "Iteration 33, loss = 0.00259012\n",
      "Iteration 34, loss = 0.00250583\n",
      "Iteration 35, loss = 0.00243726\n",
      "Iteration 36, loss = 0.00237362\n",
      "Iteration 37, loss = 0.00231138\n",
      "Iteration 38, loss = 0.00226047\n",
      "Iteration 39, loss = 0.00220886\n",
      "Iteration 40, loss = 0.00216657\n",
      "Iteration 41, loss = 0.00212219\n",
      "Iteration 42, loss = 0.00208226\n",
      "Iteration 43, loss = 0.00204524\n",
      "Iteration 44, loss = 0.00200986\n",
      "Iteration 45, loss = 0.00197698\n",
      "Iteration 46, loss = 0.00194733\n",
      "Iteration 47, loss = 0.00191815\n",
      "Iteration 48, loss = 0.00189017\n",
      "Iteration 49, loss = 0.00186606\n",
      "Iteration 50, loss = 0.00183984\n",
      "Iteration 51, loss = 0.00181661\n",
      "Iteration 52, loss = 0.00179491\n",
      "Iteration 53, loss = 0.00177345\n",
      "Iteration 54, loss = 0.00175398\n",
      "Iteration 55, loss = 0.00173517\n",
      "Iteration 56, loss = 0.00171643\n",
      "Iteration 57, loss = 0.00169953\n",
      "Iteration 58, loss = 0.00168265\n",
      "Iteration 59, loss = 0.00166656\n",
      "Iteration 60, loss = 0.00165136\n",
      "Iteration 61, loss = 0.00163745\n",
      "Iteration 62, loss = 0.00162257\n",
      "Iteration 63, loss = 0.00160976\n",
      "Iteration 64, loss = 0.00159651\n",
      "Iteration 65, loss = 0.00158494\n",
      "Iteration 66, loss = 0.00157246\n",
      "Iteration 67, loss = 0.00156161\n",
      "Iteration 68, loss = 0.00154992\n",
      "Iteration 69, loss = 0.00153920\n",
      "Iteration 70, loss = 0.00152901\n",
      "Iteration 71, loss = 0.00151954\n",
      "Iteration 72, loss = 0.00151074\n",
      "Iteration 73, loss = 0.00150152\n",
      "Iteration 74, loss = 0.00149237\n",
      "Iteration 75, loss = 0.00148376\n",
      "Iteration 76, loss = 0.00147546\n",
      "Iteration 77, loss = 0.00146790\n",
      "Iteration 78, loss = 0.00146032\n",
      "Iteration 79, loss = 0.00145284\n",
      "Iteration 80, loss = 0.00144515\n",
      "Iteration 81, loss = 0.00143835\n",
      "Iteration 82, loss = 0.00143173\n",
      "Iteration 83, loss = 0.00142531\n",
      "Iteration 84, loss = 0.00141860\n",
      "Iteration 85, loss = 0.00141336\n",
      "Iteration 86, loss = 0.00140698\n",
      "Iteration 87, loss = 0.00140097\n",
      "Iteration 88, loss = 0.00139565\n",
      "Iteration 89, loss = 0.00138990\n",
      "Iteration 90, loss = 0.00138463\n",
      "Iteration 91, loss = 0.00138009\n",
      "Iteration 92, loss = 0.00137452\n",
      "Iteration 93, loss = 0.00136945\n",
      "Iteration 94, loss = 0.00136517\n",
      "Iteration 95, loss = 0.00136057\n",
      "Iteration 96, loss = 0.00135586\n",
      "Iteration 97, loss = 0.00135152\n",
      "Iteration 98, loss = 0.00134736\n",
      "Iteration 99, loss = 0.00134327\n",
      "Iteration 100, loss = 0.00133892\n",
      "Iteration 101, loss = 0.00133515\n",
      "Iteration 102, loss = 0.00133149\n",
      "Iteration 103, loss = 0.00132796\n",
      "Iteration 104, loss = 0.00132397\n",
      "Iteration 105, loss = 0.00132046\n",
      "Iteration 106, loss = 0.00131691\n",
      "Iteration 107, loss = 0.00131344\n",
      "Iteration 108, loss = 0.00131013\n",
      "Iteration 109, loss = 0.00130673\n",
      "Iteration 110, loss = 0.00130340\n",
      "Iteration 111, loss = 0.00130044\n",
      "Iteration 112, loss = 0.00129735\n",
      "Iteration 113, loss = 0.00129427\n",
      "Iteration 114, loss = 0.00129129\n",
      "Iteration 115, loss = 0.00128834\n",
      "Iteration 116, loss = 0.00128555\n",
      "Iteration 117, loss = 0.00128299\n",
      "Iteration 118, loss = 0.00128034\n",
      "Iteration 119, loss = 0.00127754\n",
      "Iteration 120, loss = 0.00127467\n",
      "Iteration 121, loss = 0.00127208\n",
      "Iteration 122, loss = 0.00126962\n",
      "Iteration 123, loss = 0.00126708\n",
      "Iteration 124, loss = 0.00126454\n",
      "Iteration 125, loss = 0.00126210\n",
      "Iteration 126, loss = 0.00125962\n",
      "Iteration 127, loss = 0.00125718\n",
      "Iteration 128, loss = 0.00125509\n",
      "Iteration 129, loss = 0.00125270\n",
      "Iteration 130, loss = 0.00125036\n",
      "Iteration 131, loss = 0.00124807\n",
      "Iteration 132, loss = 0.00124572\n",
      "Iteration 133, loss = 0.00124337\n",
      "Iteration 134, loss = 0.00124130\n",
      "Iteration 135, loss = 0.00123905\n",
      "Iteration 136, loss = 0.00123687\n",
      "Iteration 137, loss = 0.00123473\n",
      "Iteration 138, loss = 0.00123256\n",
      "Iteration 139, loss = 0.00123057\n",
      "Iteration 140, loss = 0.00122844\n",
      "Iteration 141, loss = 0.00122639\n",
      "Iteration 142, loss = 0.00122423\n",
      "Iteration 143, loss = 0.00122207\n",
      "Iteration 144, loss = 0.00122023\n",
      "Iteration 145, loss = 0.00121841\n",
      "Iteration 146, loss = 0.00121631\n",
      "Iteration 147, loss = 0.00121424\n",
      "Iteration 148, loss = 0.00121213\n",
      "Iteration 149, loss = 0.00121013\n",
      "Iteration 150, loss = 0.00120826\n",
      "Iteration 151, loss = 0.00120609\n",
      "Iteration 152, loss = 0.00120439\n",
      "Iteration 153, loss = 0.00120242\n",
      "Iteration 154, loss = 0.00120035\n",
      "Iteration 155, loss = 0.00119827\n",
      "Iteration 156, loss = 0.00119637\n",
      "Iteration 157, loss = 0.00119463\n",
      "Iteration 158, loss = 0.00119247\n",
      "Iteration 159, loss = 0.00119067\n",
      "Iteration 160, loss = 0.00118851\n",
      "Iteration 161, loss = 0.00118697\n",
      "Iteration 162, loss = 0.00118466\n",
      "Iteration 163, loss = 0.00118294\n",
      "Iteration 164, loss = 0.00118082\n",
      "Iteration 165, loss = 0.00117891\n",
      "Iteration 166, loss = 0.00117690\n",
      "Iteration 167, loss = 0.00117486\n",
      "Iteration 168, loss = 0.00117301\n",
      "Iteration 169, loss = 0.00117095\n",
      "Iteration 170, loss = 0.00116880\n",
      "Iteration 171, loss = 0.00116704\n",
      "Iteration 172, loss = 0.00116494\n",
      "Iteration 173, loss = 0.00116297\n",
      "Iteration 174, loss = 0.00116084\n",
      "Iteration 175, loss = 0.00115923\n",
      "Iteration 176, loss = 0.00115685\n",
      "Iteration 177, loss = 0.00115433\n",
      "Iteration 178, loss = 0.00115251\n",
      "Iteration 179, loss = 0.00115036\n",
      "Iteration 180, loss = 0.00114828\n",
      "Iteration 181, loss = 0.00114606\n",
      "Iteration 182, loss = 0.00114389\n",
      "Iteration 183, loss = 0.00114162\n",
      "Iteration 184, loss = 0.00113960\n",
      "Iteration 185, loss = 0.00113740\n",
      "Iteration 186, loss = 0.00113517\n",
      "Iteration 187, loss = 0.00113281\n",
      "Iteration 188, loss = 0.00113060\n",
      "Iteration 189, loss = 0.00112819\n",
      "Iteration 190, loss = 0.00112579\n",
      "Iteration 191, loss = 0.00112342\n",
      "Iteration 192, loss = 0.00112124\n",
      "Iteration 193, loss = 0.00111881\n",
      "Iteration 194, loss = 0.00111610\n",
      "Iteration 195, loss = 0.00111377\n",
      "Iteration 196, loss = 0.00111116\n",
      "Iteration 197, loss = 0.00110884\n",
      "Iteration 198, loss = 0.00110640\n",
      "Iteration 199, loss = 0.00110383\n",
      "Iteration 200, loss = 0.00110142\n",
      "Iteration 201, loss = 0.00109884\n",
      "Iteration 202, loss = 0.00109605\n",
      "Iteration 203, loss = 0.00109331\n",
      "Iteration 204, loss = 0.00109054\n",
      "Iteration 205, loss = 0.00108810\n",
      "Iteration 206, loss = 0.00108532\n",
      "Iteration 207, loss = 0.00108234\n",
      "Iteration 208, loss = 0.00107989\n",
      "Iteration 209, loss = 0.00107700\n",
      "Iteration 210, loss = 0.00107401\n",
      "Iteration 211, loss = 0.00107101\n",
      "Iteration 212, loss = 0.00106811\n",
      "Iteration 213, loss = 0.00106520\n",
      "Iteration 214, loss = 0.00106246\n",
      "Iteration 215, loss = 0.00105946\n",
      "Iteration 216, loss = 0.00105615\n",
      "Iteration 217, loss = 0.00105334\n",
      "Iteration 218, loss = 0.00104992\n",
      "Iteration 219, loss = 0.00104691\n",
      "Iteration 220, loss = 0.00104365\n",
      "Iteration 221, loss = 0.00104042\n",
      "Iteration 222, loss = 0.00103697\n",
      "Iteration 223, loss = 0.00103408\n",
      "Iteration 224, loss = 0.00103105\n",
      "Iteration 225, loss = 0.00102744\n",
      "Iteration 226, loss = 0.00102390\n",
      "Iteration 227, loss = 0.00102058\n",
      "Iteration 228, loss = 0.00101690\n",
      "Iteration 229, loss = 0.00101356\n",
      "Iteration 230, loss = 0.00101012\n",
      "Iteration 231, loss = 0.00100647\n",
      "Iteration 232, loss = 0.00100303\n",
      "Iteration 233, loss = 0.00099936\n",
      "Iteration 234, loss = 0.00099530\n",
      "Iteration 235, loss = 0.00099201\n",
      "Iteration 236, loss = 0.00098796\n",
      "Iteration 237, loss = 0.00098441\n",
      "Iteration 238, loss = 0.00098049\n",
      "Iteration 239, loss = 0.00097632\n",
      "Iteration 240, loss = 0.00097270\n",
      "Iteration 241, loss = 0.00096981\n",
      "Iteration 242, loss = 0.00096501\n",
      "Iteration 243, loss = 0.00096108\n",
      "Iteration 244, loss = 0.00095694\n",
      "Iteration 245, loss = 0.00095267\n",
      "Iteration 246, loss = 0.00094848\n",
      "Iteration 247, loss = 0.00094414\n",
      "Iteration 248, loss = 0.00094049\n",
      "Iteration 249, loss = 0.00093635\n",
      "Iteration 250, loss = 0.00093224\n",
      "Iteration 251, loss = 0.00092817\n",
      "Iteration 252, loss = 0.00092360\n",
      "Iteration 253, loss = 1.67896385\n",
      "Iteration 254, loss = 3.77141861\n",
      "Iteration 255, loss = 2.52646539\n",
      "Iteration 256, loss = 1.68359322\n",
      "Iteration 257, loss = 1.11860560\n",
      "Iteration 258, loss = 0.77605300\n",
      "Iteration 259, loss = 0.55706030\n",
      "Iteration 260, loss = 0.31330474\n",
      "Iteration 261, loss = 0.19181229\n",
      "Iteration 262, loss = 0.16757896\n",
      "Iteration 263, loss = 0.09773397\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(1280, 630, 300), max_iter=300, tol=1e-08,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(1280, 630, 300), max_iter=300, tol=1e-08,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(1280, 630, 300), max_iter=300, tol=1e-08,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "# Scale the training and test sets using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "trainX_scaled = scaler.fit_transform(trainX)\n",
    "testX_scaled = scaler.transform(testX)\n",
    "# Train the MLPClassifier model\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(1280, 630, 300),\n",
    "                        max_iter=300, activation='relu', verbose=True, tol=1e-8,\n",
    "                        solver='adam')\n",
    "mlp_clf.fit(trainX_scaled, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc591ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test1.iloc[:,df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test1.iloc[:,0:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test1['Metadata_experiment_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a299c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new.to_csv('MLP_seen_new.csv')\n",
    "df_test1.to_csv('MLP_seen_old.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f124db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test2.iloc[:,df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test2.iloc[:,0:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test2['Metadata_experiment_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0261db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new.to_csv('MLP_unseen_new.csv')\n",
    "df_test2.to_csv('MLP_unseen_old.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9e123",
   "metadata": {},
   "source": [
    "This is SLPP Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84a44fa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2979650/3803530211.py:10: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "\n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "\n",
    "# To meet the mAP evaluation Plate\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "\n",
    "\n",
    "\n",
    "# SEEN Test\n",
    "\n",
    "df_test1=pd.concat([gene_seen,dftrain2,df_negcon]) \n",
    "#df_test1=df_test1[df_test1['Metadata_Plate'].isin(lst)]\n",
    "\n",
    "# Train\n",
    "df_train=pd.concat([train_part_total_final,dftrain2])\n",
    "\n",
    "\n",
    "### Unseen test\n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=pd.concat([df_test2,df_negcon,dftrain2])\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "#df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "y_test=df_test1['Metadata_gene']\n",
    "testX=df_test1.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec1cc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainX\n",
    "y_train = y_train.reset_index()\n",
    "y_train=y_train['Metadata_gene'].tolist()\n",
    "n_components=300\n",
    "sigma=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba04a11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2979650/1419865204.py:15: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  negative_one_idxs = np.where(labels == -1)[0]\n"
     ]
    }
   ],
   "source": [
    "## code from: github.com/tanyapole/reproduce-OSLPP/blob/main/OSLPP.py\n",
    "import scipy \n",
    "\n",
    "def get_l2_norm(features:np.ndarray): return np.sqrt(np.square(features).sum(axis=1)).reshape((-1,1))\n",
    "\n",
    "def get_l2_normalized(features:np.ndarray): return features / get_l2_norm(features)\n",
    "\n",
    "def get_PCA(features, dim):\n",
    "    result = PCA(n_components=dim).fit_transform(features)\n",
    "    assert len(features) == len(result)\n",
    "    return result\n",
    "\n",
    "def get_W(labels,):\n",
    "    W = (labels.reshape(-1,1) == labels).astype(np.int32)\n",
    "    negative_one_idxs = np.where(labels == -1)[0]\n",
    "    W[:,negative_one_idxs] = 0\n",
    "    W[negative_one_idxs,:] = 0\n",
    "    return W\n",
    "\n",
    "def get_D(W): return np.eye(len(W), dtype=np.int32) * W.sum(axis=1)\n",
    "\n",
    "def fix_numerical_assymetry(M): return (M + M.transpose()) * 0.5\n",
    "\n",
    "def get_projection_matrix(features, labels, proj_dim):\n",
    "    N, d = features.shape\n",
    "    X = features.transpose()\n",
    "    \n",
    "    W = get_W(labels)\n",
    "    D = get_D(W)\n",
    "    L = D - W\n",
    "\n",
    "    A = fix_numerical_assymetry(np.matmul(np.matmul(X, D), X.transpose()))\n",
    "    B = fix_numerical_assymetry(np.matmul(np.matmul(X, L), X.transpose()) + np.eye(d))\n",
    "    assert (A.transpose() == A).all() and (B.transpose() == B).all()\n",
    "\n",
    "    w, v = scipy.linalg.eigh(A, B)\n",
    "    assert w[0] < w[-1]\n",
    "    w, v = w[-proj_dim:], v[:, -proj_dim:]\n",
    "    assert np.abs(np.matmul(A, v) - w * np.matmul(B, v)).max() < 1e-5\n",
    "\n",
    "    w = np.flip(w)\n",
    "    v = np.flip(v, axis=1)\n",
    "\n",
    "    for i in range(v.shape[1]):\n",
    "        if v[0,i] < 0:\n",
    "            v[:,i] *= -1\n",
    "    return v\n",
    "\n",
    "def project_features(P, features):\n",
    "    # P: pca_dim x proj_dim\n",
    "    # features: N x pca_dim\n",
    "    # result: N x proj_dim\n",
    "    return np.matmul(P.transpose(), features.transpose()).transpose()\n",
    "\n",
    "y_train_array = np.array(y_train)\n",
    "P = get_projection_matrix(trainX, y_train_array, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79d8d4be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## transformed seen_test_data\n",
    "transformed_data = np.dot(testX, P)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX1=df_test1.iloc[:,:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX1=testX1.reset_index()\n",
    "dfnew =pd.concat([testX1,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7629a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.iloc[:,1:].to_csv('SLPP_seen_new.csv')          # After feature transformation\n",
    "df_test1.to_csv('SLPP_seen_old.csv')                  # Before feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e99a9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_test pre-process Unseen test data\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "y_test=df_test2['Metadata_gene']\n",
    "testX=df_test2.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)\n",
    "X_train = trainX\n",
    "y_train = y_train.reset_index()\n",
    "y_train=y_train['Metadata_gene'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "164b513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformed Unseen test Data\n",
    "transformed_data = np.dot(testX, P)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX2=df_test2.iloc[:,:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX2=testX2.reset_index()\n",
    "dfnew =pd.concat([testX2,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f183f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.iloc[:,1:].to_csv('SLPP_unseen_new.csv')\n",
    "df_test1.to_csv('SLPP_unseen_old.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853c358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71820f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d260e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
