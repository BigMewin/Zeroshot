{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ae7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c78a69b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2287118/2561710930.py:3: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "\n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "\n",
    "# To meet the mAP evaluation Plate\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "df_test1=gene_seen\n",
    "\n",
    "# train\n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) \n",
    "\n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "df_test2=df_test2[df_test2['Metadata_experiment_type']=='Compound']\n",
    "df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "# SEEN\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "df_test1_rm_emptygene=df_test1[df_test1['Metadata_gene']!='empty']\n",
    "y_test=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efdfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc9148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "# Scale the training and test sets using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "trainX_scaled = scaler.fit_transform(trainX)\n",
    "testX_scaled = scaler.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978b6c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.65383336\n",
      "Iteration 2, loss = 3.79941345\n",
      "Iteration 3, loss = 3.16271248\n",
      "Iteration 4, loss = 2.61318419\n",
      "Iteration 5, loss = 2.18646016\n",
      "Iteration 6, loss = 1.76074004\n",
      "Iteration 7, loss = 1.41503976\n",
      "Iteration 8, loss = 1.08378473\n",
      "Iteration 9, loss = 0.81721168\n",
      "Iteration 10, loss = 0.60477527\n",
      "Iteration 11, loss = 0.47835066\n",
      "Iteration 12, loss = 0.37031966\n",
      "Iteration 13, loss = 0.27941140\n",
      "Iteration 14, loss = 0.22490581\n",
      "Iteration 15, loss = 0.15779994\n",
      "Iteration 16, loss = 0.13894733\n",
      "Iteration 17, loss = 0.13147846\n",
      "Iteration 18, loss = 0.14978283\n",
      "Iteration 19, loss = 0.09638828\n",
      "Iteration 20, loss = 0.07939811\n",
      "Iteration 21, loss = 0.04851315\n",
      "Iteration 22, loss = 0.04382160\n",
      "Iteration 23, loss = 0.04270304\n",
      "Iteration 24, loss = 0.02543963\n",
      "Iteration 25, loss = 0.01560932\n",
      "Iteration 26, loss = 0.04048562\n",
      "Iteration 27, loss = 0.01992309\n",
      "Iteration 28, loss = 0.00934599\n",
      "Iteration 29, loss = 0.00471831\n",
      "Iteration 30, loss = 0.00281415\n",
      "Iteration 31, loss = 0.00237852\n",
      "Iteration 32, loss = 0.00221927\n",
      "Iteration 33, loss = 0.00211625\n",
      "Iteration 34, loss = 0.00203747\n",
      "Iteration 35, loss = 0.00197289\n",
      "Iteration 36, loss = 0.00191999\n",
      "Iteration 37, loss = 0.00187472\n",
      "Iteration 38, loss = 0.00183499\n",
      "Iteration 39, loss = 0.00179925\n",
      "Iteration 40, loss = 0.00176751\n",
      "Iteration 41, loss = 0.00173986\n",
      "Iteration 42, loss = 0.00171281\n",
      "Iteration 43, loss = 0.00168922\n",
      "Iteration 44, loss = 0.00166806\n",
      "Iteration 45, loss = 0.00164738\n",
      "Iteration 46, loss = 0.00162869\n",
      "Iteration 47, loss = 0.00161114\n",
      "Iteration 48, loss = 0.00159534\n",
      "Iteration 49, loss = 0.00158027\n",
      "Iteration 50, loss = 0.00156552\n",
      "Iteration 51, loss = 0.00155240\n",
      "Iteration 52, loss = 0.00153912\n",
      "Iteration 53, loss = 0.00152730\n",
      "Iteration 54, loss = 0.00151583\n",
      "Iteration 55, loss = 0.00150484\n",
      "Iteration 56, loss = 0.00149477\n",
      "Iteration 57, loss = 0.00148517\n",
      "Iteration 58, loss = 0.00147622\n",
      "Iteration 59, loss = 0.00146708\n",
      "Iteration 60, loss = 0.00145805\n",
      "Iteration 61, loss = 0.00144975\n",
      "Iteration 62, loss = 0.00144229\n",
      "Iteration 63, loss = 0.00143627\n",
      "Iteration 64, loss = 0.00142748\n",
      "Iteration 65, loss = 0.00142144\n",
      "Iteration 66, loss = 0.00141451\n",
      "Iteration 67, loss = 0.00140805\n",
      "Iteration 68, loss = 0.00140266\n",
      "Iteration 69, loss = 0.00139671\n",
      "Iteration 70, loss = 0.00139093\n",
      "Iteration 71, loss = 0.00138629\n",
      "Iteration 72, loss = 0.00138201\n",
      "Iteration 73, loss = 0.00137596\n",
      "Iteration 74, loss = 0.00137131\n",
      "Iteration 75, loss = 0.00136679\n",
      "Iteration 76, loss = 0.00136190\n",
      "Iteration 77, loss = 0.00135802\n",
      "Iteration 78, loss = 0.00135392\n",
      "Iteration 79, loss = 0.00134968\n",
      "Iteration 80, loss = 0.00134618\n",
      "Iteration 81, loss = 0.00134181\n",
      "Iteration 82, loss = 0.00133805\n",
      "Iteration 83, loss = 0.00133435\n",
      "Iteration 84, loss = 0.00133111\n",
      "Iteration 85, loss = 0.00132796\n",
      "Iteration 86, loss = 0.00132435\n",
      "Iteration 87, loss = 0.00132124\n",
      "Iteration 88, loss = 0.00131814\n",
      "Iteration 89, loss = 0.00131519\n",
      "Iteration 90, loss = 0.00131205\n",
      "Iteration 91, loss = 0.00130928\n",
      "Iteration 92, loss = 0.00130632\n",
      "Iteration 93, loss = 0.00130349\n",
      "Iteration 94, loss = 0.00130096\n",
      "Iteration 95, loss = 0.00129832\n",
      "Iteration 96, loss = 0.00129572\n",
      "Iteration 97, loss = 0.00129322\n",
      "Iteration 98, loss = 0.00129076\n",
      "Iteration 99, loss = 0.00128853\n",
      "Iteration 100, loss = 0.00128626\n",
      "Iteration 101, loss = 0.00128361\n",
      "Iteration 102, loss = 0.00128128\n",
      "Iteration 103, loss = 0.00127916\n",
      "Iteration 104, loss = 0.00127694\n",
      "Iteration 105, loss = 0.00127482\n",
      "Iteration 106, loss = 0.00127303\n",
      "Iteration 107, loss = 0.00127080\n",
      "Iteration 108, loss = 0.00126864\n",
      "Iteration 109, loss = 0.00126675\n",
      "Iteration 110, loss = 0.00126488\n",
      "Iteration 111, loss = 0.00126309\n",
      "Iteration 112, loss = 0.00126111\n",
      "Iteration 113, loss = 0.00125901\n",
      "Iteration 114, loss = 0.00125727\n",
      "Iteration 115, loss = 0.00125547\n",
      "Iteration 116, loss = 0.00125391\n",
      "Iteration 117, loss = 0.00125204\n",
      "Iteration 118, loss = 0.00125049\n",
      "Iteration 119, loss = 0.00124852\n",
      "Iteration 120, loss = 0.00124721\n",
      "Iteration 121, loss = 0.00124573\n",
      "Iteration 122, loss = 0.00124361\n",
      "Iteration 123, loss = 0.00124202\n",
      "Iteration 124, loss = 0.00124036\n",
      "Iteration 125, loss = 0.00123874\n",
      "Iteration 126, loss = 0.00123726\n",
      "Iteration 127, loss = 0.00123573\n",
      "Iteration 128, loss = 0.00123433\n",
      "Iteration 129, loss = 0.00123257\n",
      "Iteration 130, loss = 0.00123120\n",
      "Iteration 131, loss = 0.00122977\n",
      "Iteration 132, loss = 0.00122833\n",
      "Iteration 133, loss = 0.00122668\n",
      "Iteration 134, loss = 0.00122545\n",
      "Iteration 135, loss = 0.00122395\n",
      "Iteration 136, loss = 0.00122241\n",
      "Iteration 137, loss = 0.00122131\n",
      "Iteration 138, loss = 0.00121966\n",
      "Iteration 139, loss = 0.00121834\n",
      "Iteration 140, loss = 0.00121686\n",
      "Iteration 141, loss = 0.00121558\n",
      "Iteration 142, loss = 0.00121425\n",
      "Iteration 143, loss = 0.00121282\n",
      "Iteration 144, loss = 0.00121149\n",
      "Iteration 145, loss = 0.00121026\n",
      "Iteration 146, loss = 0.00120879\n",
      "Iteration 147, loss = 0.00120765\n",
      "Iteration 148, loss = 0.00120624\n",
      "Iteration 149, loss = 0.00120491\n",
      "Iteration 150, loss = 0.00120363\n",
      "Iteration 151, loss = 0.00120274\n",
      "Iteration 152, loss = 0.00120109\n",
      "Iteration 153, loss = 0.00119971\n",
      "Iteration 154, loss = 0.00119836\n",
      "Iteration 155, loss = 0.00119709\n",
      "Iteration 156, loss = 0.00119586\n",
      "Iteration 157, loss = 0.00119465\n",
      "Iteration 158, loss = 0.00119325\n",
      "Iteration 159, loss = 0.00119207\n",
      "Iteration 160, loss = 0.00119079\n",
      "Iteration 161, loss = 0.00118947\n",
      "Iteration 162, loss = 0.00118829\n",
      "Iteration 163, loss = 0.00118698\n",
      "Iteration 164, loss = 0.00118573\n",
      "Iteration 165, loss = 0.00118440\n",
      "Iteration 166, loss = 0.00118317\n",
      "Iteration 167, loss = 0.00118190\n",
      "Iteration 168, loss = 0.00118070\n",
      "Iteration 169, loss = 0.00117956\n",
      "Iteration 170, loss = 0.00117815\n",
      "Iteration 171, loss = 0.00117686\n",
      "Iteration 172, loss = 0.00117561\n",
      "Iteration 173, loss = 0.00117431\n",
      "Iteration 174, loss = 0.00117308\n",
      "Iteration 175, loss = 0.00117173\n",
      "Iteration 176, loss = 0.00117046\n",
      "Iteration 177, loss = 0.00116919\n",
      "Iteration 178, loss = 0.00116788\n",
      "Iteration 179, loss = 0.00116654\n",
      "Iteration 180, loss = 0.00116534\n",
      "Iteration 181, loss = 0.00116401\n",
      "Iteration 182, loss = 0.00116262\n",
      "Iteration 183, loss = 0.00116123\n",
      "Iteration 184, loss = 0.00115990\n",
      "Iteration 185, loss = 0.00115858\n",
      "Iteration 186, loss = 0.00115719\n",
      "Iteration 187, loss = 0.00115589\n",
      "Iteration 188, loss = 0.00115450\n",
      "Iteration 189, loss = 0.00115322\n",
      "Iteration 190, loss = 0.00115181\n",
      "Iteration 191, loss = 0.00115039\n",
      "Iteration 192, loss = 0.00114904\n",
      "Iteration 193, loss = 0.00114765\n",
      "Iteration 194, loss = 0.00114639\n",
      "Iteration 195, loss = 0.00114489\n",
      "Iteration 196, loss = 0.00114354\n",
      "Iteration 197, loss = 0.00114204\n",
      "Iteration 198, loss = 0.00114080\n",
      "Iteration 199, loss = 0.00113929\n",
      "Iteration 200, loss = 0.00113799\n",
      "Iteration 201, loss = 0.00113656\n",
      "Iteration 202, loss = 0.00113500\n",
      "Iteration 203, loss = 0.00113355\n",
      "Iteration 204, loss = 0.00113217\n",
      "Iteration 205, loss = 0.00113074\n",
      "Iteration 206, loss = 0.00112933\n",
      "Iteration 207, loss = 0.00112789\n",
      "Iteration 208, loss = 0.00112643\n",
      "Iteration 209, loss = 0.00112496\n",
      "Iteration 210, loss = 0.00112355\n",
      "Iteration 211, loss = 0.00112192\n",
      "Iteration 212, loss = 0.00112034\n",
      "Iteration 213, loss = 0.00111890\n",
      "Iteration 214, loss = 0.00111741\n",
      "Iteration 215, loss = 0.00111588\n",
      "Iteration 216, loss = 0.00111436\n",
      "Iteration 217, loss = 0.00111286\n",
      "Iteration 218, loss = 0.00111130\n",
      "Iteration 219, loss = 0.00110969\n",
      "Iteration 220, loss = 0.00110813\n",
      "Iteration 221, loss = 0.00110670\n",
      "Iteration 222, loss = 0.00110500\n",
      "Iteration 223, loss = 0.00110325\n",
      "Iteration 224, loss = 0.00110168\n",
      "Iteration 225, loss = 0.00109999\n",
      "Iteration 226, loss = 0.00109847\n",
      "Iteration 227, loss = 0.00109656\n",
      "Iteration 228, loss = 0.00109498\n",
      "Iteration 229, loss = 0.00109322\n",
      "Iteration 230, loss = 0.00109150\n",
      "Iteration 231, loss = 0.00108976\n",
      "Iteration 232, loss = 0.00108806\n",
      "Iteration 233, loss = 0.00108632\n",
      "Iteration 234, loss = 0.00108445\n",
      "Iteration 235, loss = 0.00108266\n",
      "Iteration 236, loss = 0.00108097\n",
      "Iteration 237, loss = 0.00107900\n",
      "Iteration 238, loss = 0.00107709\n",
      "Iteration 239, loss = 0.00107530\n",
      "Iteration 240, loss = 0.00107339\n",
      "Iteration 241, loss = 0.00107155\n",
      "Iteration 242, loss = 0.00106970\n",
      "Iteration 243, loss = 0.00106779\n",
      "Iteration 244, loss = 0.00106566\n",
      "Iteration 245, loss = 0.00106368\n",
      "Iteration 246, loss = 0.00106182\n",
      "Iteration 247, loss = 0.00105994\n",
      "Iteration 248, loss = 0.00105785\n",
      "Iteration 249, loss = 0.00105590\n",
      "Iteration 250, loss = 0.00105375\n",
      "Iteration 251, loss = 0.00105168\n",
      "Iteration 252, loss = 0.00104960\n",
      "Iteration 253, loss = 0.00104756\n",
      "Iteration 254, loss = 0.00104537\n",
      "Iteration 255, loss = 0.00104338\n",
      "Iteration 256, loss = 0.00104140\n",
      "Iteration 257, loss = 0.00103917\n",
      "Iteration 258, loss = 0.00103681\n",
      "Iteration 259, loss = 0.00103473\n",
      "Iteration 260, loss = 0.00103228\n",
      "Iteration 261, loss = 0.00103007\n",
      "Iteration 262, loss = 0.00102793\n",
      "Iteration 263, loss = 0.00102554\n",
      "Iteration 264, loss = 0.00102321\n",
      "Iteration 265, loss = 0.00102101\n",
      "Iteration 266, loss = 0.00101852\n",
      "Iteration 267, loss = 0.00101607\n",
      "Iteration 268, loss = 0.00101366\n",
      "Iteration 269, loss = 0.00101122\n",
      "Iteration 270, loss = 0.00100885\n",
      "Iteration 271, loss = 0.00100638\n",
      "Iteration 272, loss = 0.00100397\n",
      "Iteration 273, loss = 0.00100137\n",
      "Iteration 274, loss = 0.00099879\n",
      "Iteration 275, loss = 0.00099629\n",
      "Iteration 276, loss = 0.00099377\n",
      "Iteration 277, loss = 0.00099117\n",
      "Iteration 278, loss = 0.00098850\n",
      "Iteration 279, loss = 0.00098583\n",
      "Iteration 280, loss = 0.00098316\n",
      "Iteration 281, loss = 0.00098037\n",
      "Iteration 282, loss = 0.00097767\n",
      "Iteration 283, loss = 0.00097492\n",
      "Iteration 284, loss = 0.00097236\n",
      "Iteration 285, loss = 0.00096943\n",
      "Iteration 286, loss = 0.00096657\n",
      "Iteration 287, loss = 0.00096367\n",
      "Iteration 288, loss = 0.00096096\n",
      "Iteration 289, loss = 0.00095795\n",
      "Iteration 290, loss = 0.00095523\n",
      "Iteration 291, loss = 0.00095218\n",
      "Iteration 292, loss = 0.00094936\n",
      "Iteration 293, loss = 0.00094654\n",
      "Iteration 294, loss = 0.00094335\n",
      "Iteration 295, loss = 0.00094017\n",
      "Iteration 296, loss = 0.00093717\n",
      "Iteration 297, loss = 0.00093393\n",
      "Iteration 298, loss = 0.00093087\n",
      "Iteration 299, loss = 0.00092771\n",
      "Iteration 300, loss = 0.00092435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(1280, 630, 300, 176), max_iter=300, tol=1e-08,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(1280, 630, 300, 176), max_iter=300, tol=1e-08,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(1280, 630, 300, 176), max_iter=300, tol=1e-08,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "# Scale the training and test sets using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "trainX_scaled = scaler.fit_transform(trainX)\n",
    "testX_scaled = scaler.transform(testX)\n",
    "\n",
    "# Train the MLPClassifier model\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(1280, 630, 300, 176),\n",
    "                        max_iter=300, activation='relu', verbose=True, tol=1e-8,\n",
    "                        solver='adam')\n",
    "mlp_clf.fit(trainX_scaled, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f52d60",
   "metadata": {},
   "source": [
    "These below are MLP Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ec86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plug in mode = 'top_1' if you want to apply normal MLP classification\n",
    "# plug in mode = 'top_10' if you want to get the top 10 highest \n",
    "def Compute_MLP_Result(trainX,testX,testY,trainY,mode):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import numpy as np\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report, recall_score\n",
    "    if mode == 'top_1':\n",
    "    # MLP TOP ONE (Not TOP TEN)\n",
    "        sc=StandardScaler()\n",
    "        scaler = sc.fit(trainX)\n",
    "        trainX_scaled = scaler.transform(trainX)\n",
    "        testX_scaled = scaler.transform(testX)\n",
    "        y_pred = mlp_clf.predict(testX_scaled)\n",
    "    #print('Seen')\n",
    "    #print(classification_report(testY, y_pred))\n",
    "        report=classification_report(testY, y_pred,output_dict=True)\n",
    "        result=pd.DataFrame(report).transpose()[['recall']]\n",
    "        result.rename(columns={'recall': 'Recall'},inplace=True)\n",
    "        result.drop(['micro avg','macro avg','weighted avg'],axis=0,errors='ignore',inplace = True)\n",
    "        result.index.name='Label'\n",
    "        result=result[result.index.isin(pd.Series(testY).unique())]\n",
    "        return result\n",
    "    if mode =='top_10':\n",
    "        # MLP Top Ten\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        sc = StandardScaler()\n",
    "        sc.fit(testX)\n",
    "        le = LabelEncoder()\n",
    "        trainY_encoded = le.fit_transform(trainY)\n",
    "        testY_encoded = le.transform(testY)\n",
    "        y_pred_probs = mlp_clf.predict_proba(sc.transform(testX))\n",
    "\n",
    "# Initialize a dictionary to keep track of the number of correct predictions for each label\n",
    "        label_counts = {label: {'total': 0, 'correct': 0} for label in set(testY)}\n",
    "\n",
    "# Loop over each sample in the test set\n",
    "        for i in range(len(testY)):\n",
    "            true_label = testY[i]\n",
    "            true_label_encoded = testY_encoded[i]\n",
    "            top_ten_labels_encoded = np.argsort(y_pred_probs[i])[::-1][:10]\n",
    "            top_ten_labels = le.inverse_transform(top_ten_labels_encoded)\n",
    "            if true_label in top_ten_labels:\n",
    "                label_counts[true_label]['correct'] += 1\n",
    "            label_counts[true_label]['total'] += 1\n",
    "\n",
    "# Calculate the accuracy for each label and print the results\n",
    "        for label, counts in label_counts.items():\n",
    "            if counts['total'] > 0:\n",
    "                accuracy = counts['correct'] / counts['total']\n",
    "                print(f\"Label {label}: {accuracy:.2f} ({counts['correct']}/{counts['total']})\")\n",
    "            else:\n",
    "                print(f\"Label {label}: No samples in test set\")\n",
    "\n",
    "# Calculate the accuracy for each label and store the results in a dictionary\n",
    "        accuracy_dict = {}\n",
    "        for label, counts in label_counts.items():\n",
    "            if counts['total'] > 0:\n",
    "                accuracy = counts['correct'] / counts['total']\n",
    "                accuracy_dict[label] = accuracy\n",
    "\n",
    "# Create a DataFrame with the accuracy for each label\n",
    "        accuracy_df = pd.DataFrame.from_dict(accuracy_dict, orient='index', columns=['Accuracy'])\n",
    "        accuracy_df.index.name = 'Label'\n",
    "        accuracy_df = accuracy_df.sort_values(by='Accuracy', ascending=False)\n",
    "        return accuracy_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e96e0dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label ICAM1: 0.80 (12/15)\n",
      "Label CACNB4: 0.06 (1/16)\n",
      "Label AKR1B1: 0.31 (5/16)\n",
      "Label GHSR: 0.44 (7/16)\n",
      "Label HTR3A: 0.94 (15/16)\n",
      "Label EDNRB: 0.44 (7/16)\n",
      "Label HSD11B1: 0.38 (6/16)\n",
      "Label CYP1A2: 0.06 (1/16)\n",
      "Label PTK2B: 0.00 (0/16)\n",
      "Label ALK: 0.81 (13/16)\n",
      "Label GLRA3: 0.06 (1/16)\n",
      "Label KCNK1: 0.38 (6/16)\n",
      "Label P3H1: 0.33 (5/15)\n",
      "Label CSF1R: 0.00 (0/15)\n",
      "Label HCK: 1.00 (16/16)\n",
      "Label BTK: 0.62 (10/16)\n",
      "Label KCNQ2: 0.75 (12/16)\n",
      "Label CYP3A4: 0.93 (14/15)\n",
      "Label TGFBR1: 0.38 (6/16)\n",
      "Label CACNA2D3: 0.13 (2/15)\n",
      "Label GJB4: 0.94 (15/16)\n",
      "Label PPARD: 0.31 (5/16)\n",
      "Label PLA2G1B: 0.38 (6/16)\n",
      "Label GPR55: 0.00 (0/15)\n",
      "Label PPAT: 0.50 (8/16)\n",
      "Label KCNJ1: 0.80 (12/15)\n",
      "Label GAA: 0.44 (7/16)\n",
      "Label ADRA2B: 0.25 (4/16)\n",
      "Label SLC29A1: 0.25 (4/16)\n",
      "Label ELANE: 0.44 (7/16)\n",
      "Label RPL3: 0.50 (8/16)\n",
      "Label DCK: 0.56 (9/16)\n",
      "Label VEGFA: 0.19 (3/16)\n",
      "Label HSP90AB1: 0.94 (15/16)\n",
      "Label RNASE1: 0.88 (14/16)\n",
      "Label SIRT2: 0.88 (14/16)\n",
      "Label CASP3: 0.33 (5/15)\n",
      "Label UGT1A9: 0.87 (13/15)\n",
      "Label HIF1A: 0.40 (6/15)\n",
      "Label DHH: 0.53 (8/15)\n",
      "Label COMT: 0.50 (8/16)\n",
      "Label MME: 0.47 (7/15)\n",
      "Label PDPK1: 0.80 (12/15)\n",
      "Label PDE3A: 0.50 (8/16)\n",
      "Label KCNN1: 0.06 (1/16)\n",
      "Label AGER: 0.62 (10/16)\n",
      "Label TNF: 0.25 (4/16)\n",
      "Label CTSG: 0.06 (1/16)\n",
      "Label KCNMA1: 0.75 (12/16)\n",
      "Label FFAR4: 0.69 (11/16)\n",
      "Label GPR119: 0.75 (12/16)\n",
      "Label FPR1: 0.69 (11/16)\n",
      "Label HTR2C: 0.44 (7/16)\n",
      "Label CA5A: 0.87 (13/15)\n",
      "Label TNNC1: 0.75 (12/16)\n",
      "Label SLCO2B1: 0.44 (7/16)\n",
      "Label IL1B: 0.44 (7/16)\n",
      "Label OPRM1: 0.38 (6/16)\n",
      "Label FFAR2: 0.81 (13/16)\n",
      "Label F10: 0.25 (4/16)\n",
      "Label HPGDS: 0.60 (9/15)\n",
      "Label PRKCB: 0.56 (9/16)\n",
      "Label MAPK8: 0.40 (6/15)\n",
      "Label CDK4: 0.38 (6/16)\n",
      "Label CCND1: 0.62 (10/16)\n",
      "Label PDE7A: 0.44 (7/16)\n",
      "Label SCNN1G: 0.50 (8/16)\n",
      "Label HRH4: 0.44 (7/16)\n",
      "Label S1PR2: 0.25 (4/16)\n",
      "Label NTRK1: 0.06 (1/16)\n",
      "Label KCTD16: 0.88 (14/16)\n",
      "Label TBXAS1: 0.44 (7/16)\n",
      "Label TUBB4B: 1.00 (16/16)\n",
      "Label ADH1C: 0.31 (5/16)\n",
      "Label P2RY12: 0.62 (10/16)\n",
      "Label FGF1: 0.44 (7/16)\n",
      "Label PLD1: 0.88 (14/16)\n",
      "Label ITGB2: 0.56 (9/16)\n",
      "Label OPRL1: 1.00 (16/16)\n",
      "Label PNLIP: 0.50 (8/16)\n",
      "Label ASIC1: 0.31 (5/16)\n",
      "Label DDR2: 0.44 (7/16)\n",
      "Label BAX: 0.56 (9/16)\n",
      "Label CYP2A6: 0.81 (13/16)\n",
      "Label KDR: 0.62 (10/16)\n",
      "Label PDE4D: 0.94 (15/16)\n",
      "Label LCK: 0.88 (14/16)\n",
      "Label ADA: 0.00 (0/16)\n",
      "Label ADORA2A: 0.20 (3/15)\n",
      "Label RPL23A: 0.94 (15/16)\n",
      "Label CATSPER4: 0.50 (8/16)\n",
      "Label HDAC6: 0.25 (4/16)\n",
      "Label SSTR2: 0.62 (10/16)\n",
      "Label AKR1C1: 0.07 (1/15)\n",
      "Label KCNH7: 0.31 (5/16)\n",
      "Label ATP5F1D: 0.13 (2/15)\n",
      "Label CHRM3: 0.56 (9/16)\n",
      "Label PORCN: 0.53 (8/15)\n",
      "Label CHRM2: 0.50 (8/16)\n",
      "Label GRIN2A: 0.07 (1/15)\n",
      "Label CSK: 0.27 (4/15)\n"
     ]
    }
   ],
   "source": [
    "# top1 / Top 10 Result Seen\n",
    "top1_seen_result=Compute_MLP_Result(trainX,testX,testY,trainY,'top_1')\n",
    "top10_seen_result=Compute_MLP_Result(trainX,testX,testY,trainY,'top_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c80c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UNSEEN data Preparasion\n",
    "dfte2_rmempty=df_test2[df_test2['Metadata_gene']!='empty']\n",
    "y_test=dfte2_rmempty[dfte2_rmempty['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=dfte2_rmempty[dfte2_rmempty['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX_final=testX\n",
    "testY_final=y_test\n",
    "testX=np.array(testX)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2759a525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label CACNG1: 0.00 (0/31)\n",
      "Label ALDH2: 0.00 (0/32)\n",
      "Label SLC7A11: 0.00 (0/31)\n",
      "Label HBB: 0.00 (0/32)\n",
      "Label RGS4: 0.19 (6/32)\n",
      "Label GUCY1B1: 0.00 (0/31)\n",
      "Label MMP2: 0.06 (2/32)\n",
      "Label S100B: 0.06 (2/31)\n",
      "Label ATM: 0.00 (0/32)\n",
      "Label S1PR4: 0.00 (0/30)\n",
      "Label PRKCE: 0.37 (11/30)\n",
      "Label S1PR1: 0.00 (0/31)\n",
      "Label LYN: 0.50 (16/32)\n",
      "Label ABL1: 0.19 (6/32)\n",
      "Label CDK9: 0.41 (13/32)\n",
      "Label CDC25A: 0.03 (1/31)\n",
      "Label TGM2: 0.00 (0/32)\n",
      "Label TUBB3: 0.34 (11/32)\n",
      "Label KCNN4: 0.00 (0/31)\n",
      "Label CA14: 0.03 (1/32)\n",
      "Label CCR1: 0.00 (0/32)\n",
      "Label ANXA1: 0.19 (6/31)\n",
      "Label LPAR1: 0.31 (10/32)\n",
      "Label PTGIR: 0.00 (0/32)\n",
      "Label AVPR1A: 0.00 (0/32)\n",
      "Label PTGIS: 0.00 (0/31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# top1 / Top 10 Result Unseen\n",
    "top1_unseen_result=Compute_MLP_Result(trainX,testX,testY,trainY,'top_1')\n",
    "top10_unseen_result=Compute_MLP_Result(trainX,testX,testY,trainY,'top_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50584786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 AVG Seen ACC MLP 0.1757013201320132\n",
      "Top1 AVG Unseen ACC MLP 0.004807692307692308\n",
      "Top1 AVG ALL ACC MLP 0.14071522309711287\n"
     ]
    }
   ],
   "source": [
    "#Summary of top 1 result MLP\n",
    "top1_seen_result['If_seen']='seen'\n",
    "top1_unseen_result['If_seen']='unseen'\n",
    "all_top1_result=pd.concat([top1_seen_result,top1_unseen_result])\n",
    "all_top1_result=all_top1_result.dropna()\n",
    "print('Top1 AVG Seen ACC MLP',top1_seen_result['Recall'].mean())\n",
    "print('Top1 AVG Unseen ACC MLP',top1_unseen_result['Recall'].mean())\n",
    "print('Top1 AVG ALL ACC MLP',all_top1_result['Recall'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b8abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top10 AVG Seen ACC MLP 0.4959983498349835\n",
      "Top10 AVG Unseen ACC MLP 0.10339381720430109\n",
      "Top10 AVG ALL ACC MLP 0.4156226187452375\n"
     ]
    }
   ],
   "source": [
    "#Summary of top 10 result MLP\n",
    "top10_seen_result['If_seen']='seen'\n",
    "top10_unseen_result['If_seen']='unseen'\n",
    "top10_all_result=pd.concat([top10_seen_result,top10_unseen_result])\n",
    "top10_all_result=top10_all_result.dropna()\n",
    "print('Top10 AVG Seen ACC MLP',top10_seen_result['Accuracy'].mean())\n",
    "print('Top10 AVG Unseen ACC MLP',top10_unseen_result['Accuracy'].mean())\n",
    "print('Top10 AVG ALL ACC MLP',top10_all_result['Accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce66f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f719edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a4f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation for Test Set Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_proceed.iloc[:,df_proceed.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_proceed.iloc[:,0:df_proceed.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_proceed['Metadata_experiment_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd52eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new=feature_new.iloc[:,1:-1]\n",
    "compound_new=feature_new[feature_new['Metadata_experiment_type']=='Compound']\n",
    "compound_new.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/Compound.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a031137",
   "metadata": {},
   "outputs": [],
   "source": [
    "##These are used to train CCVAE\n",
    "CRISPR_new=feature_new[feature_new['Metadata_experiment_type']=='CRISPR']\n",
    "CRISPR_new.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/CRISPR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8635e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new=feature_new.reset_index()\n",
    "df_test1=df_test1.reset_index()\n",
    "df_test2=df_test2.reset_index()\n",
    "df_test1=df_test1.iloc[:,2:]\n",
    "df_test2=df_test2.iloc[:,2:]\n",
    "feature_new=feature_new.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d143350",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test1.iloc[:,df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test1.iloc[:,0:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test1['Metadata_experiment_type']], axis=1)\n",
    "dfte1a=feature_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a162be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test2.iloc[:,df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test2.iloc[:,0:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test2['Metadata_experiment_type']], axis=1)\n",
    "dfte2a=feature_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252b4afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21f5b44",
   "metadata": {},
   "source": [
    "These below is for 1-NN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77bef5a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_train.iloc[:,df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_train.iloc[:,0:df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_train['Metadata_experiment_type']], axis=1)\n",
    "dftrain_new=feature_new\n",
    "\n",
    "# Modify the shape of data with transformed features\n",
    "y_train=dftrain_new['Metadata_gene']\n",
    "trainX=dftrain_new.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(0):]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "y_test=dfte1a['Metadata_gene']\n",
    "testX=dfte1a.drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(0):]\n",
    "testX=testX.iloc[:,:-1]\n",
    "\n",
    "## prepare for training\n",
    "trainX=trainX.iloc[:,:-1]\n",
    "trainX=np.array(trainX)\n",
    "trainY=y_train.to_list()\n",
    "trainY=np.array(trainY)\n",
    "testX=np.array(testX)\n",
    "testY=np.array(y_test)\n",
    "k=130\n",
    "label_vectors=trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ad7bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def kNCM(trainX, trainY, testX, testY, k):\n",
    "    # Convert the label strings to integers\n",
    "    label_to_int = {label: i for i, label in enumerate(set(trainY))}\n",
    "    trainY_int = np.array([label_to_int[label] for label in trainY])\n",
    "    testY_int = np.array([label_to_int[label] for label in testY])\n",
    "\n",
    "    # Compute the mean vectors for each class in the training set\n",
    "    class_means = {}\n",
    "    for label in set(trainY_int):\n",
    "        class_means[label] = np.mean(trainX[trainY_int == label], axis=0)\n",
    "\n",
    "    # Use the class means as the label vectors\n",
    "    label_vectors = np.array(list(class_means.values()))\n",
    "\n",
    "    # Classify the test samples using the k-nearest class mean algorithm\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn_clf.fit(label_vectors, list(class_means.keys()))\n",
    "    nn_indices = knn_clf.kneighbors(testX, return_distance=False)\n",
    "\n",
    "    # Compute the predicted labels for each test sample\n",
    "    top_ten_labels_int = np.array([[knn_clf.classes_[i] for i in nn] for nn in nn_indices])\n",
    "\n",
    "    # Convert the label indices to label names and filter out invalid labels\n",
    "    int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "    top_ten_labels = np.vectorize(lambda x: int_to_label.get(x, None))(top_ten_labels_int)\n",
    "    top_ten_labels = np.array([labels[labels != None][:10] for labels in top_ten_labels])\n",
    "\n",
    "    # Merge testY and top_ten_labels into a DataFrame\n",
    "    df = pd.DataFrame({'testY': testY, 'top_ten_labels': top_ten_labels.tolist()})\n",
    "    lst=[]\n",
    "    for i in range(len(df)):\n",
    "        if df['testY'][i] in df['top_ten_labels'][i]:\n",
    "            score=1\n",
    "        else:\n",
    "            score=0\n",
    "        lst.append(score)\n",
    "    df['score']=lst\n",
    "    avg_df = df.groupby('testY')['score'].mean().reset_index()\n",
    "    avg_df=avg_df.sort_values(by='score',ascending= False)\n",
    "    return avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc3605f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNCM_seen_result= kNCM(trainX, trainY, testX, testY, k)\n",
    "KNCM_seen_result['If_seen']='seen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26afed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Unseen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_train.iloc[:,df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_train.iloc[:,0:df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_train['Metadata_experiment_type']], axis=1)\n",
    "dftrain_new=feature_new\n",
    "\n",
    "##### Modify the shape and contents of data transformed new feature\n",
    "y_train=dftrain_new['Metadata_gene']\n",
    "trainX=dftrain_new.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(0):]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "y_test=dfte2a['Metadata_gene']\n",
    "testX=dfte2a.drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(0):]\n",
    "testX=testX.iloc[:,:-1]\n",
    "\n",
    "## Prepare for training \n",
    "trainX=trainX.iloc[:,:-1]\n",
    "trainX=np.array(trainX)\n",
    "trainY=y_train.to_list()\n",
    "trainY=np.array(trainY)\n",
    "testX=np.array(testX)\n",
    "testY=np.array(y_test)\n",
    "k=130\n",
    "label_vectors=trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03c1eb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_KNCM_SEEN 0.3752475247524753\n",
      "MLP_KNCM_UNEEN 0.14375258478081057\n",
      "MLP_KNCM_TOTAL 0.32785485987638646\n"
     ]
    }
   ],
   "source": [
    "KNCM_unseen_result= kNCM(trainX, trainY, testX, testY, k)\n",
    "KNCM_unseen_result['If_seen']='unseen'\n",
    "KNCM_total_result=pd.concat([KNCM_seen_result,KNCM_unseen_result])\n",
    "print('MLP_KNCM_SEEN',KNCM_seen_result['score'].mean())\n",
    "print('MLP_KNCM_UNEEN',KNCM_unseen_result['score'].mean())\n",
    "print('MLP_KNCM_TOTAL',KNCM_total_result['score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382a5a2",
   "metadata": {},
   "source": [
    "This is for SLPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50599ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2287118/2561710930.py:3: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "\n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "\n",
    "# To meet the mAP evaluation Plate\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "df_test1=gene_seen\n",
    "\n",
    "# train\n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) \n",
    "\n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "df_test2=df_test2[df_test2['Metadata_experiment_type']=='Compound']\n",
    "df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "# SEEN\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "df_test1_rm_emptygene=df_test1[df_test1['Metadata_gene']!='empty']\n",
    "y_test=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91f27821",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=300\n",
    "sigma=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51d3eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # SLPP Training eigenvectors\n",
    "    # Standardize the data\n",
    "    X_train = StandardScaler().fit_transform(trainX)\n",
    "    y_train=trainY\n",
    "    # Compute the pairwise distance matrix\n",
    "    D = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            D[i, j] = np.linalg.norm(X_train[i] - X_train[j])\n",
    "\n",
    "    # Compute the adjacency graph using a Gaussian kernel\n",
    "    W = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            W[i, j] = np.exp(-D[i, j] ** 2 / (2 * sigma ** 2)) * (y_train[i] == y_train[j])\n",
    "\n",
    "    # Compute the degree matrix\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "\n",
    "    # Compute the Laplacian matrix\n",
    "    L = D - W\n",
    "\n",
    "    # Compute the eigenvectors\n",
    "    eigvals, eigvecs = np.linalg.eig(np.dot(np.dot(X_train.T, L), X_train))\n",
    "\n",
    "    # Sort the eigenvectors by eigenvalues in descending order\n",
    "    sort_indices = np.argsort(eigvals)[::-1]\n",
    "    eigvecs = eigvecs[:, sort_indices]\n",
    "\n",
    "    # Select the first n_components eigenvectors\n",
    "    eigvecs = eigvecs[:, :n_components]\n",
    "    eigvecs=np.real(eigvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3a45e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training data feature  from 904 to 300 dimension\n",
    "transformed_data = np.dot(df_train.iloc[:,15:], eigvecs)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "trainX1=df_train.iloc[:,:df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "trainX1=trainX1.reset_index()\n",
    "dftrain_trans =pd.concat([trainX1,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed7f7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform SEEN test data feature  from 904 to 300 dimension\n",
    "transformed_data = np.dot(testX, eigvecs)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX1=df_test1.iloc[:,:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX1=testX1.reset_index()\n",
    "dfnew1 =pd.concat([testX1,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba9e253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training KNCM\n",
    "y_train=dftrain_trans['Metadata_gene']\n",
    "trainX=dftrain_trans.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(0):]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "y_test=dfnew1['Metadata_gene']\n",
    "testX=dfnew1.drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(0):]\n",
    "trainY=y_train.tolist()\n",
    "testY=y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fb84fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=130\n",
    "KNCM_seen_result= kNCM(trainX, trainY, testX, testY, k)\n",
    "KNCM_seen_result['If_seen']='seen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caa33829",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is for Unseen Test data preparasion\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "df_test2_rm_emptygene=df_test2[df_test2['Metadata_gene']!='empty']\n",
    "y_test=df_test2_rm_emptygene[df_test2_rm_emptygene['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=df_test2_rm_emptygene[df_test2_rm_emptygene['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)\n",
    "X_train = trainX\n",
    "y_train = trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0cd68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature transformation on Unseen test set\n",
    "transformed_data = np.dot(testX, eigvecs)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX2=df_test2.iloc[:,:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX2=testX2.reset_index()\n",
    "dfnew2 =pd.concat([testX2,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12b881ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Prepare for training\n",
    "y_train=dftrain_trans['Metadata_gene']\n",
    "trainX=dftrain_trans.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(0):]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "y_test=dfnew2['Metadata_gene']\n",
    "testX=dfnew2.drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(0):]\n",
    "trainY=y_train.tolist()\n",
    "testY=y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f684c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLPP_KNCM_SEEN 0.3182343234323432\n",
      "SLPP_KNCM_UNEEN 0.07201457816377171\n",
      "SLPP_KNCM_TOTAL 0.2678271314875963\n"
     ]
    }
   ],
   "source": [
    "KNCM_unseen_result= kNCM(trainX, trainY, testX, testY, k)\n",
    "KNCM_unseen_result['If_seen']='unseen'\n",
    "KNCM_total_result=pd.concat([KNCM_seen_result,KNCM_unseen_result])\n",
    "print('SLPP_KNCM_SEEN',KNCM_seen_result['score'].mean())\n",
    "print('SLPP_KNCM_UNEEN',KNCM_unseen_result['score'].mean())\n",
    "print('SLPP_KNCM_TOTAL',KNCM_total_result['score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c894e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
