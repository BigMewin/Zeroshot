{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ae7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5322c93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177713/2697063891.py:3: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/total_compound.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         test_lst\u001b[38;5;241m.\u001b[39mappend(m)\n\u001b[1;32m     25\u001b[0m df_compound\u001b[38;5;241m=\u001b[39mdf_proceed[df_proceed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetadata_experiment_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompound\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m df_compound\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/total_compound.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 80% target gene of Compound + Gene perturbation \u001b[39;00m\n\u001b[1;32m     29\u001b[0m dftrain1\u001b[38;5;241m=\u001b[39mdf_compound[df_compound[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetadata_gene\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(train_lst)]  \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3718\u001b[0m )\n\u001b[0;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[1;32m   3721\u001b[0m     path_or_buf,\n\u001b[1;32m   3722\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   3723\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   3724\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   3725\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   3726\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   3727\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[1;32m   3728\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   3729\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[1;32m   3730\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   3731\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m   3732\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[1;32m   3733\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[1;32m   3734\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[1;32m   3735\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[1;32m   3736\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   3737\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    244\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    245\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[1;32m    246\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[1;32m    248\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/total_compound.csv'"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "df_compound.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/total_compound.csv')\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "\n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "dftrain2.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/total_CRISPR.csv')\n",
    "# To meet the mAP evaluation Plate\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1])#.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "train_part_total_final=train_part_total_final.drop_duplicates()\n",
    "df_test1=gene_seen\n",
    "df_test1=df_test1[df_test1['Metadata_Plate'].isin(lst)]\n",
    "# train\n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) \n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "df_test2=df_test2[df_test2['Metadata_experiment_type']=='Compound']\n",
    "df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "# SEEN\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "df_test1_rm_emptygene=df_test1[df_test1['Metadata_gene']!='empty']\n",
    "y_test=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)\n",
    "df_proceed = df_proceed[df_proceed['Metadata_Plate'].isin(lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "# Scale the training and test sets using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "trainX_scaled = scaler.fit_transform(trainX)\n",
    "testX_scaled = scaler.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978b6c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Scale the training and test sets using StandardScaler\u001b[39;00m\n\u001b[1;32m      7\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 8\u001b[0m trainX_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(trainX)\n\u001b[1;32m      9\u001b[0m testX_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(testX)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the MLPClassifier model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "# Scale the training and test sets using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "trainX_scaled = scaler.fit_transform(trainX)\n",
    "testX_scaled = scaler.transform(testX)\n",
    "\n",
    "# Train the MLPClassifier model\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(1280, 630, 300),\n",
    "                        max_iter=30, activation='relu', verbose=True, tol=1e-8,\n",
    "                        solver='adam')\n",
    "mlp_clf.fit(trainX_scaled, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f52d60",
   "metadata": {},
   "source": [
    "These below are MLP Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ec86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plug in mode = 'top_1' if you want to apply normal MLP classification\n",
    "# plug in mode = 'top_10' if you want to get the top 10 highest \n",
    "def Compute_MLP_Result(trainX,testX,testY,trainY,mode):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import numpy as np\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report, recall_score\n",
    "    if mode == 'top_1':\n",
    "    # MLP TOP ONE (Not TOP TEN)\n",
    "        sc=StandardScaler()\n",
    "        scaler = sc.fit(trainX)\n",
    "        trainX_scaled = scaler.transform(trainX)\n",
    "        testX_scaled = scaler.transform(testX)\n",
    "        y_pred = mlp_clf.predict(testX_scaled)\n",
    "    #print('Seen')\n",
    "    #print(classification_report(testY, y_pred))\n",
    "        report=classification_report(testY, y_pred,output_dict=True)\n",
    "        result=pd.DataFrame(report).transpose()[['recall']]\n",
    "        result.rename(columns={'recall': 'Recall'},inplace=True)\n",
    "        result.drop(['micro avg','macro avg','weighted avg'],axis=0,errors='ignore',inplace = True)\n",
    "        result.index.name='Label'\n",
    "        result=result[result.index.isin(pd.Series(testY).unique())]\n",
    "        return result\n",
    "    if mode =='top_10':\n",
    "        # MLP Top Ten\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        sc = StandardScaler()\n",
    "        sc.fit(testX)\n",
    "        le = LabelEncoder()\n",
    "        trainY_encoded = le.fit_transform(trainY)\n",
    "        testY_encoded = le.transform(testY)\n",
    "        y_pred_probs = mlp_clf.predict_proba(sc.transform(testX))\n",
    "\n",
    "# Initialize a dictionary to keep track of the number of correct predictions for each label\n",
    "        label_counts = {label: {'total': 0, 'correct': 0} for label in set(testY)}\n",
    "\n",
    "# Loop over each sample in the test set\n",
    "        for i in range(len(testY)):\n",
    "            true_label = testY[i]\n",
    "            true_label_encoded = testY_encoded[i]\n",
    "            top_ten_labels_encoded = np.argsort(y_pred_probs[i])[::-1][:10]\n",
    "            top_ten_labels = le.inverse_transform(top_ten_labels_encoded)\n",
    "            if true_label in top_ten_labels:\n",
    "                label_counts[true_label]['correct'] += 1\n",
    "            label_counts[true_label]['total'] += 1\n",
    "\n",
    "# Calculate the accuracy for each label and print the results\n",
    "        for label, counts in label_counts.items():\n",
    "            if counts['total'] > 0:\n",
    "                accuracy = counts['correct'] / counts['total']\n",
    "                print(f\"Label {label}: {accuracy:.2f} ({counts['correct']}/{counts['total']})\")\n",
    "            else:\n",
    "                print(f\"Label {label}: No samples in test set\")\n",
    "\n",
    "# Calculate the accuracy for each label and store the results in a dictionary\n",
    "        accuracy_dict = {}\n",
    "        for label, counts in label_counts.items():\n",
    "            if counts['total'] > 0:\n",
    "                accuracy = counts['correct'] / counts['total']\n",
    "                accuracy_dict[label] = accuracy\n",
    "\n",
    "# Create a DataFrame with the accuracy for each label\n",
    "        accuracy_df = pd.DataFrame.from_dict(accuracy_dict, orient='index', columns=['Accuracy'])\n",
    "        accuracy_df.index.name = 'Label'\n",
    "        accuracy_df = accuracy_df.sort_values(by='Accuracy', ascending=False)\n",
    "        return accuracy_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96e0dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# top1 / Top 10 Result Seen\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m top1_seen_result\u001b[38;5;241m=\u001b[39mCompute_MLP_Result(trainX,testX,testY,trainY,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m top10_seen_result\u001b[38;5;241m=\u001b[39mCompute_MLP_Result(trainX,testX,testY,trainY,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_10\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "# top1 / Top 10 Result Seen\n",
    "top1_seen_result=Compute_MLP_Result(trainX,testX,testY,trainY,'top_1')\n",
    "top10_seen_result=Compute_MLP_Result(trainX,testX,testY,trainY,'top_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c80c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UNSEEN data Preparasion\n",
    "dfte2_rmempty=df_test2[df_test2['Metadata_gene']!='empty']\n",
    "y_test=dfte2_rmempty[dfte2_rmempty['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=dfte2_rmempty[dfte2_rmempty['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX_final=testX\n",
    "testY_final=y_test\n",
    "testX=np.array(testX)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2759a525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuchen.yang@insilico.ai/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label PTGIR: 0.00 (0/32)\n",
      "Label AVPR1A: 0.00 (0/32)\n",
      "Label RGS4: 0.19 (6/32)\n",
      "Label ABL1: 0.03 (1/32)\n",
      "Label SLC7A11: 0.00 (0/31)\n",
      "Label KCNN4: 0.00 (0/31)\n",
      "Label S100B: 0.03 (1/31)\n",
      "Label HBB: 0.06 (2/32)\n",
      "Label ATM: 0.00 (0/32)\n",
      "Label GUCY1B1: 0.03 (1/31)\n",
      "Label CDK9: 0.44 (14/32)\n",
      "Label ANXA1: 0.00 (0/31)\n",
      "Label MMP2: 0.03 (1/32)\n",
      "Label ALDH2: 0.03 (1/32)\n",
      "Label PTGIS: 0.00 (0/31)\n",
      "Label PRKCE: 0.27 (8/30)\n",
      "Label LYN: 0.22 (7/32)\n",
      "Label CCR1: 0.00 (0/32)\n",
      "Label TUBB3: 0.47 (15/32)\n",
      "Label S1PR1: 0.00 (0/31)\n",
      "Label CACNG1: 0.06 (2/31)\n",
      "Label S1PR4: 0.03 (1/30)\n",
      "Label CDC25A: 0.10 (3/31)\n",
      "Label CA14: 0.09 (3/32)\n",
      "Label TGM2: 0.03 (1/32)\n",
      "Label LPAR1: 0.19 (6/32)\n"
     ]
    }
   ],
   "source": [
    "# top1 / Top 10 Result Unseen\n",
    "top1_unseen_result=Compute_MLP_Result(trainX,testX,testY,trainY,'top_1')\n",
    "top10_unseen_result=Compute_MLP_Result(trainX,testX,testY,trainY,'top_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50584786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 AVG Seen ACC MLP 0.3313118811881188\n",
      "Top1 AVG Unseen ACC MLP 0.001201923076923077\n",
      "Top1 AVG ALL ACC MLP 0.26373031496062993\n"
     ]
    }
   ],
   "source": [
    "#Summary of top 1 result MLP\n",
    "top1_seen_result['If_seen']='seen'\n",
    "top1_unseen_result['If_seen']='unseen'\n",
    "all_top1_result=pd.concat([top1_seen_result,top1_unseen_result])\n",
    "all_top1_result=all_top1_result.dropna()\n",
    "print('Top1 AVG Seen ACC MLP',top1_seen_result['Recall'].mean())\n",
    "print('Top1 AVG Unseen ACC MLP',top1_unseen_result['Recall'].mean())\n",
    "print('Top1 AVG ALL ACC MLP',all_top1_result['Recall'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86b8abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top10 AVG Seen ACC MLP 0.7100660066006601\n",
      "Top10 AVG Unseen ACC MLP 0.08873294044665013\n",
      "Top10 AVG ALL ACC MLP 0.5828639615612565\n"
     ]
    }
   ],
   "source": [
    "#Summary of top 10 result MLP\n",
    "top10_seen_result['If_seen']='seen'\n",
    "top10_unseen_result['If_seen']='unseen'\n",
    "top10_all_result=pd.concat([top10_seen_result,top10_unseen_result])\n",
    "top10_all_result=top10_all_result.dropna()\n",
    "print('Top10 AVG Seen ACC MLP',top10_seen_result['Accuracy'].mean())\n",
    "print('Top10 AVG Unseen ACC MLP',top10_unseen_result['Accuracy'].mean())\n",
    "print('Top10 AVG ALL ACC MLP',top10_all_result['Accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34a4f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation for Test Set Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=pd.concat([df_test1,df_test2]).iloc[:,pd.concat([df_test1,df_test2]).columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=pd.concat([df_test1,df_test2]).iloc[:,0:pd.concat([df_test1,df_test2]).columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,pd.concat([df_test1,df_test2])['Metadata_experiment_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ad6e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new=feature_new.iloc[:,1:-1]\n",
    "compound_new=feature_new[feature_new['Metadata_experiment_type']=='Compound']\n",
    "compound_new=compound_new[compound_new['Metadata_Plate'].isin(lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb8fc937",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_new.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/Compound.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6e63557",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cris=df_train[df_train['Metadata_experiment_type']=='CRISPR']\n",
    "##### Feature Transformation for Train CRISPR \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_cris.iloc[:,df_cris.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_cris.iloc[:,0:df_cris.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "CRISPR_new =pd.concat([transformen_seen,feature,df_cris['Metadata_experiment_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c27a31e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRISPR_new.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/CRISPR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8635e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new=feature_new.reset_index()\n",
    "df_test1=df_test1.reset_index()\n",
    "df_test2=df_test2.reset_index()\n",
    "df_test1=df_test1.iloc[:,2:]\n",
    "df_test2=df_test2.iloc[:,2:]\n",
    "feature_new=feature_new.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d143350",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test1.iloc[:,df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test1.iloc[:,0:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test1['Metadata_experiment_type']], axis=1)\n",
    "dfte1a=feature_new\n",
    "dfte1a.to_csv('MLP_seen_ccvae.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a162be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_test2.iloc[:,df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_test2.iloc[:,0:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_test2['Metadata_experiment_type']], axis=1)\n",
    "dfte2a=feature_new\n",
    "dfte2a.to_csv('MLP_unseen_ccvae.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493dd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21f5b44",
   "metadata": {},
   "source": [
    "These below is for 1-NN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77bef5a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Feature Transformation Seen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_train.iloc[:,df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_train.iloc[:,0:df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_train['Metadata_experiment_type']], axis=1)\n",
    "dftrain_new=feature_new\n",
    "\n",
    "# Modify the shape of data with transformed features\n",
    "y_train=dftrain_new['Metadata_gene']\n",
    "trainX=dftrain_new.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(0):]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "y_test=dfte1a['Metadata_gene']\n",
    "testX=dfte1a.drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(0):]\n",
    "testX=testX.iloc[:,:-1]\n",
    "\n",
    "## prepare for training\n",
    "trainX=trainX.iloc[:,:-1]\n",
    "trainX=np.array(trainX)\n",
    "trainY=y_train.to_list()\n",
    "trainY=np.array(trainY)\n",
    "testX=np.array(testX)\n",
    "testY=np.array(y_test)\n",
    "k=130\n",
    "label_vectors=trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ad7bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def kNCM(trainX, trainY, testX, testY, k, mode):\n",
    "    # Convert the label strings to integers\n",
    "    label_to_int = {label: i for i, label in enumerate(set(trainY))}\n",
    "    trainY_int = np.array([label_to_int[label] for label in trainY])\n",
    "    testY_int = np.array([label_to_int[label] for label in testY])\n",
    "\n",
    "    # Compute the mean vectors for each class in the training set\n",
    "    class_means = {}\n",
    "    for label in set(trainY_int):\n",
    "        class_means[label] = np.mean(trainX[trainY_int == label], axis=0)\n",
    "\n",
    "    # Use the class means as the label vectors\n",
    "    label_vectors = np.array(list(class_means.values()))\n",
    "\n",
    "    # Classify the test samples using the k-nearest class mean algorithm\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn_clf.fit(label_vectors, list(class_means.keys()))\n",
    "    nn_indices = knn_clf.kneighbors(testX, return_distance=False)\n",
    "\n",
    "    # Compute the predicted labels for each test sample\n",
    "    top_ten_labels_int = np.array([[knn_clf.classes_[i] for i in nn] for nn in nn_indices])\n",
    "\n",
    "    # Convert the label indices to label names and filter out invalid labels\n",
    "    int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "    top_ten_labels = np.vectorize(lambda x: int_to_label.get(x, None))(top_ten_labels_int)\n",
    "    if mode == 'top_1':\n",
    "        top_ten_labels = np.array([labels[labels != None][:1] for labels in top_ten_labels])\n",
    "    if mode == 'top_10':\n",
    "        top_ten_labels = np.array([labels[labels != None][:10] for labels in top_ten_labels])\n",
    "\n",
    "    # Merge testY and top_ten_labels into a DataFrame\n",
    "    df = pd.DataFrame({'testY': testY, 'top_ten_labels': top_ten_labels.tolist()})\n",
    "    lst=[]\n",
    "    for i in range(len(df)):\n",
    "        if df['testY'][i] in df['top_ten_labels'][i]:\n",
    "            score=1\n",
    "        else:\n",
    "            score=0\n",
    "        lst.append(score)\n",
    "    df['score']=lst\n",
    "    avg_df = df.groupby('testY')['score'].mean().reset_index()\n",
    "    avg_df=avg_df.sort_values(by='score',ascending= False)\n",
    "    return avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc3605f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNCM_seen_result= kNCM(trainX, trainY, testX, testY, k,'top_10')\n",
    "KNCM_seen_result['If_seen']='seen'\n",
    "KNCM_seen_result1= kNCM(trainX, trainY, testX, testY, k,'top_1')\n",
    "KNCM_seen_result1['If_seen']='seen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26afed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Transformation Unseen \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "relu = nn.ReLU(inplace=True)\n",
    "original_seen=df_train.iloc[:,df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "sc=StandardScaler()\n",
    "scaler = sc.fit(original_seen)\n",
    "original_seen_scaled = scaler.transform(original_seen)\n",
    "layer_1=np.array(relu(torch.from_numpy(np.dot(original_seen_scaled,mlp_clf.coefs_[0])+mlp_clf.intercepts_[0])))\n",
    "layer_2=np.array(relu(torch.from_numpy(np.dot(layer_1,mlp_clf.coefs_[1])+mlp_clf.intercepts_[1])))\n",
    "layer_3=np.array(relu(torch.from_numpy(np.dot(layer_2,mlp_clf.coefs_[2])+mlp_clf.intercepts_[2])))\n",
    "transformen_seen=df_train.iloc[:,0:df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "feature=pd.DataFrame(layer_3)\n",
    "feature.index=transformen_seen.index\n",
    "feature_new =pd.concat([transformen_seen,feature,df_train['Metadata_experiment_type']], axis=1)\n",
    "dftrain_new=feature_new\n",
    "\n",
    "##### Modify the shape and contents of data transformed new feature\n",
    "y_train=dftrain_new['Metadata_gene']\n",
    "trainX=dftrain_new.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(0):]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "y_test=dfte2a['Metadata_gene']\n",
    "testX=dfte2a.drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(0):]\n",
    "testX=testX.iloc[:,:-1]\n",
    "\n",
    "## Prepare for training \n",
    "trainX=trainX.iloc[:,:-1]\n",
    "trainX=np.array(trainX)\n",
    "trainY=y_train.to_list()\n",
    "trainY=np.array(trainY)\n",
    "testX=np.array(testX)\n",
    "testY=np.array(y_test)\n",
    "k=130\n",
    "label_vectors=trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "03c1eb44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_KNCM_SEEN_TOP10 0.5833333333333334\n",
      "MLP_KNCM_UNEEN_TOP10 0.14747983870967743\n",
      "MLP_KNCM_TOTAL_TOP10 0.4941034840403014\n",
      "\n",
      "MLP_KNCM_SEEN_TOP1 0.18679867986798682\n",
      "MLP_KNCM_UNEEN_TOP1 0.03032464846980976\n",
      "MLP_KNCM_TOTAL_TOP1 0.4941034840403014\n"
     ]
    }
   ],
   "source": [
    "KNCM_unseen_result= kNCM(trainX, trainY, testX, testY, k,'top_10')\n",
    "KNCM_unseen_result['If_seen']='unseen'\n",
    "KNCM_total_result=pd.concat([KNCM_seen_result,KNCM_unseen_result])\n",
    "print('MLP_KNCM_SEEN_TOP10',KNCM_seen_result['score'].mean())\n",
    "print('MLP_KNCM_UNEEN_TOP10',KNCM_unseen_result['score'].mean())\n",
    "print('MLP_KNCM_TOTAL_TOP10',KNCM_total_result['score'].mean())\n",
    "KNCM_unseen_result1= kNCM(trainX, trainY, testX, testY, k,'top_1')\n",
    "KNCM_unseen_result1['If_seen']='unseen'\n",
    "KNCM_total_result1=pd.concat([KNCM_seen_result,KNCM_unseen_result])\n",
    "print('')\n",
    "print('MLP_KNCM_SEEN_TOP1',KNCM_seen_result1['score'].mean())\n",
    "print('MLP_KNCM_UNEEN_TOP1',KNCM_unseen_result1['score'].mean())\n",
    "print('MLP_KNCM_TOTAL_TOP1',KNCM_total_result1['score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079b8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7382a5a2",
   "metadata": {},
   "source": [
    "This is for SLPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50599ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177713/1081510986.py:3: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "#df_compound.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/total_compound.csv')\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "\n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "#dftrain2.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/total_CRISPR.csv')\n",
    "# To meet the mAP evaluation Plate\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1])#.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "train_part_total_final=train_part_total_final.drop_duplicates()\n",
    "df_test1=gene_seen\n",
    "df_test1=df_test1[df_test1['Metadata_Plate'].isin(lst)]\n",
    "# train\n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) \n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "df_test2=df_test2[df_test2['Metadata_experiment_type']=='Compound']\n",
    "df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "# SEEN\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "df_test1_rm_emptygene=df_test1[df_test1['Metadata_gene']!='empty']\n",
    "y_test=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)\n",
    "df_proceed = df_proceed[df_proceed['Metadata_Plate'].isin(lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "91f27821",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=300\n",
    "sigma=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "51d3eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # SLPP Training eigenvectors\n",
    "#     # Standardize the data\n",
    "#     X_train = StandardScaler().fit_transform(trainX)\n",
    "#     y_train=trainY\n",
    "#     # Compute the pairwise distance matrix\n",
    "#     D = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "#     for i in range(X_train.shape[0]):\n",
    "#         for j in range(X_train.shape[0]):\n",
    "#             D[i, j] = np.linalg.norm(X_train[i] - X_train[j])\n",
    "\n",
    "#     # Compute the adjacency graph using a Gaussian kernel\n",
    "#     W = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "#     for i in range(X_train.shape[0]):\n",
    "#         for j in range(X_train.shape[0]):\n",
    "#             W[i, j] = np.exp(-D[i, j] ** 2 / (2 * sigma ** 2)) * (y_train[i] == y_train[j])\n",
    "\n",
    "#     # Compute the degree matrix\n",
    "#     D = np.diag(np.sum(W, axis=1))\n",
    "\n",
    "#     # Compute the Laplacian matrix\n",
    "#     L = D - W\n",
    "\n",
    "#     # Compute the eigenvectors\n",
    "#     eigvals, eigvecs = np.linalg.eig(np.dot(np.dot(X_train.T, L), X_train))\n",
    "\n",
    "#     # Sort the eigenvectors by eigenvalues in descending order\n",
    "#     sort_indices = np.argsort(eigvals)[::-1]\n",
    "#     eigvecs = eigvecs[:, sort_indices]\n",
    "\n",
    "#     # Select the first n_components eigenvectors\n",
    "#     eigvecs = eigvecs[:, :n_components]\n",
    "#     eigvecs=np.real(eigvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7e4041a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code from: github.com/tanyapole/reproduce-OSLPP/blob/main/OSLPP.py\n",
    "import scipy \n",
    "\n",
    "def get_l2_norm(features:np.ndarray): return np.sqrt(np.square(features).sum(axis=1)).reshape((-1,1))\n",
    "\n",
    "def get_l2_normalized(features:np.ndarray): return features / get_l2_norm(features)\n",
    "\n",
    "def get_PCA(features, dim):\n",
    "    result = PCA(n_components=dim).fit_transform(features)\n",
    "    assert len(features) == len(result)\n",
    "    return result\n",
    "\n",
    "def get_W(labels,):\n",
    "    W = (labels.reshape(-1,1) == labels).astype(np.int32)\n",
    "    negative_one_idxs = np.where(labels == -1)[0]\n",
    "    W[:,negative_one_idxs] = 0\n",
    "    W[negative_one_idxs,:] = 0\n",
    "    return W\n",
    "\n",
    "def get_D(W): return np.eye(len(W), dtype=np.int32) * W.sum(axis=1)\n",
    "\n",
    "def fix_numerical_assymetry(M): return (M + M.transpose()) * 0.5\n",
    "\n",
    "def get_projection_matrix(features, labels, proj_dim):\n",
    "    N, d = features.shape\n",
    "    X = features.transpose()\n",
    "    \n",
    "    W = get_W(labels)\n",
    "    D = get_D(W)\n",
    "    L = D - W\n",
    "\n",
    "    A = fix_numerical_assymetry(np.matmul(np.matmul(X, D), X.transpose()))\n",
    "    B = fix_numerical_assymetry(np.matmul(np.matmul(X, L), X.transpose()) + np.eye(d))\n",
    "    assert (A.transpose() == A).all() and (B.transpose() == B).all()\n",
    "\n",
    "    w, v = scipy.linalg.eigh(A, B)\n",
    "    assert w[0] < w[-1]\n",
    "    w, v = w[-proj_dim:], v[:, -proj_dim:]\n",
    "    assert np.abs(np.matmul(A, v) - w * np.matmul(B, v)).max() < 1e-5\n",
    "\n",
    "    w = np.flip(w)\n",
    "    v = np.flip(v, axis=1)\n",
    "\n",
    "    for i in range(v.shape[1]):\n",
    "        if v[0,i] < 0:\n",
    "            v[:,i] *= -1\n",
    "    return v\n",
    "\n",
    "def project_features(P, features):\n",
    "    # P: pca_dim x proj_dim\n",
    "    # features: N x pca_dim\n",
    "    # result: N x proj_dim\n",
    "    return np.matmul(P.transpose(), features.transpose()).transpose()\n",
    "\n",
    "y_train_array = np.array(y_train)\n",
    "P = get_projection_matrix(X_train, y_train_array, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c3a45e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training data feature  from 904 to 300 dimension\n",
    "transformed_data = np.dot(df_train.iloc[:,15:], P)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "trainX1=df_train.iloc[:,:df_train.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "trainX1=trainX1.reset_index()\n",
    "dftrain_trans =pd.concat([trainX1,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed7f7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform SEEN test data feature  from 904 to 300 dimension\n",
    "transformed_data = np.dot(testX, P)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX1=df_test1.iloc[:,:df_test1.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX1=testX1.reset_index()\n",
    "dfnew1 =pd.concat([testX1,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ba9e253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training KNCM\n",
    "y_train=dftrain_trans['Metadata_gene']\n",
    "trainX=dftrain_trans.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(0):]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "y_test=dfnew1['Metadata_gene']\n",
    "testX=dfnew1.drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(0):]\n",
    "trainY=y_train.tolist()\n",
    "testY=y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fb84fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=130\n",
    "KNCM_seen_result= kNCM(trainX, trainY, testX, testY, k,'top_10')\n",
    "KNCM_seen_result['If_seen']='seen'\n",
    "KNCM_seen_result1= kNCM(trainX, trainY, testX, testY, k,'top_1')\n",
    "KNCM_seen_result1['If_seen']='seen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "caa33829",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is for Unseen Test data preparasion\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "df_test2_rm_emptygene=df_test2[df_test2['Metadata_gene']!='empty']\n",
    "y_test=df_test2_rm_emptygene[df_test2_rm_emptygene['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=df_test2_rm_emptygene[df_test2_rm_emptygene['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)\n",
    "X_train = trainX\n",
    "y_train = trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b0cd68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature transformation on Unseen test set\n",
    "transformed_data = np.dot(testX, P)\n",
    "new_feature=pd.DataFrame(transformed_data)\n",
    "testX2=df_test2.iloc[:,:df_test2.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\")]\n",
    "testX2=testX2.reset_index()\n",
    "dfnew2 =pd.concat([testX2,new_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "12b881ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Prepare for training\n",
    "y_train=dftrain_trans['Metadata_gene']\n",
    "trainX=dftrain_trans.drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(0):]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "y_test=dfnew2['Metadata_gene']\n",
    "testX=dfnew2.drop('Metadata_gene', axis = 1)\n",
    "testX=testX.iloc[:,testX.columns.get_loc(0):]\n",
    "trainY=y_train.tolist()\n",
    "testY=y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9f684c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLPP_KNCM_SEEN_TOP10 0.5527227722772278\n",
      "SLPP_KNCM_UNEEN_TOP10 0.03148521505376344\n",
      "SLPP_KNCM_TOTAL_TOP10 0.44601272119210905\n",
      "\n",
      "SLPP_KNCM_SEEN_TOP1 0.22652640264026405\n",
      "SLPP_KNCM_UNEEN_TOP1 0.008413461538461538\n",
      "SLPP_KNCM_TOTAL_TOP1 0.18187335958005252\n"
     ]
    }
   ],
   "source": [
    "KNCM_unseen_result= kNCM(trainX, trainY, testX, testY, k,'top_10')\n",
    "KNCM_unseen_result['If_seen']='unseen'\n",
    "KNCM_total_result=pd.concat([KNCM_seen_result,KNCM_unseen_result])\n",
    "print('SLPP_KNCM_SEEN_TOP10',KNCM_seen_result['score'].mean())\n",
    "print('SLPP_KNCM_UNEEN_TOP10',KNCM_unseen_result['score'].mean())\n",
    "print('SLPP_KNCM_TOTAL_TOP10',KNCM_total_result['score'].mean())\n",
    "KNCM_unseen_result1= kNCM(trainX, trainY, testX, testY, k,'top_1')\n",
    "KNCM_unseen_result1['If_seen']='unseen'\n",
    "KNCM_total_result1=pd.concat([KNCM_seen_result1,KNCM_unseen_result1])\n",
    "print('')\n",
    "print('SLPP_KNCM_SEEN_TOP1',KNCM_seen_result1['score'].mean())\n",
    "print('SLPP_KNCM_UNEEN_TOP1',KNCM_unseen_result1['score'].mean())\n",
    "print('SLPP_KNCM_TOTAL_TOP1',KNCM_total_result1['score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e24fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca99bfed",
   "metadata": {},
   "source": [
    "This is for Cellprofiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e5690ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_308837/1081510986.py:3: DtypeWarning: Columns (2,6,7,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "file_path='total_new.csv'\n",
    "df=pd.read_csv(file_path)\n",
    "df[\"Metadata_gene\"]=df[\"Metadata_gene\"].fillna('empty')\n",
    "df_fill_empty=df\n",
    "df_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']=='negcon']\n",
    "df_rm_negcon=df_fill_empty[df_fill_empty['Metadata_control_type']!='negcon']\n",
    "df_rm_empty_gene=df_fill_empty[df_fill_empty['Metadata_gene']!='empty']\n",
    "df_rm_orf=df_rm_empty_gene[df_rm_empty_gene[\"Metadata_experiment_type\"]!='ORF']\n",
    "# sampling\n",
    "df_proceed=df_rm_orf[~df_rm_orf['Metadata_pert_type'].isin(['control'])]\n",
    "gene_num=(df_proceed['Metadata_gene'].value_counts())\n",
    "gene_num_list=gene_num.index.tolist()\n",
    "# Only in list of plate is for mAP classifiation, so we filter it out\n",
    "lst=['BR00116991','BR00116992','BR00116993','BR00116994','BR00117015','BR00117016','BR00117017','BR00117019','BR00116995','BR00117024','BR00117025','BR00117026','BR00117010','BR00117011','BR00117012','BR00117013']\n",
    "\n",
    "# 8:2 Split Target Gene\n",
    "random.seed(1)\n",
    "train_lst=random.sample(gene_num_list, 104)#####\n",
    "test_lst = []\n",
    "for m in gene_num_list:\n",
    "    if m not in train_lst:\n",
    "        test_lst.append(m)\n",
    "        \n",
    "df_compound=df_proceed[df_proceed['Metadata_experiment_type']=='Compound']\n",
    "#df_compound.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/total_compound.csv')\n",
    "# 80% target gene of Compound + Gene perturbation \n",
    "\n",
    "dftrain1=df_compound[df_compound['Metadata_gene'].isin(train_lst)]  \n",
    "\n",
    "# 100% CRISPR for Train\n",
    "dftrain2=df_proceed[df_proceed['Metadata_experiment_type']=='CRISPR']    \n",
    "#dftrain2.to_csv('/data/datacenter/H3C_GPU/projects/yuchen/gzsda-main/gzsda-main/data/XrayBaggage20/total_CRISPR.csv')\n",
    "# To meet the mAP evaluation Plate\n",
    "train_part_total=pd.DataFrame()\n",
    "for i in lst:\n",
    "    train_part_total=pd.concat([train_part_total,dftrain1])#.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "train_part0=pd.DataFrame()\n",
    "for i in ['BR00116991','BR00116992','BR00116993','BR00116994']:\n",
    "    train_part0=pd.concat([train_part0,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part1=pd.DataFrame()\n",
    "for i in ['BR00117015','BR00117016','BR00117017','BR00117019']:\n",
    "    train_part1=pd.concat([train_part1,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part2=pd.DataFrame()\n",
    "for i in ['BR00116995','BR00117024','BR00117025','BR00117026']:\n",
    "    train_part2=pd.concat([train_part2,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])\n",
    "    \n",
    "train_part3=pd.DataFrame()\n",
    "for i in ['BR00117010','BR00117011','BR00117012','BR00117013']:\n",
    "    train_part3=pd.concat([train_part3,dftrain1.loc[dftrain1['Metadata_Plate']==i,:]])  \n",
    "    \n",
    "# test data sampling\n",
    "gene_seen=pd.DataFrame()\n",
    "for i in train_lst:\n",
    "    train_part0_filtered=train_part0.loc[train_part0['Metadata_gene']==i,:]\n",
    "    train_part1_filtered=train_part1.loc[train_part1['Metadata_gene']==i,:]\n",
    "    train_part2_filtered=train_part2.loc[train_part2['Metadata_gene']==i,:]\n",
    "    train_part3_filtered=train_part3.loc[train_part3['Metadata_gene']==i,:]\n",
    "\n",
    "# To filter seen/unseen gene from compound perturbation\n",
    "\n",
    "    if len(train_part0_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part0_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part0_filtered[train_part0_filtered['Metadata_pert_iname']==filter_out]])\n",
    "        \n",
    "    if len(train_part1_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part1_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part1_filtered[train_part1_filtered['Metadata_pert_iname']==filter_out]]) \n",
    "        \n",
    "    if len(train_part2_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part2_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part2_filtered[train_part2_filtered['Metadata_pert_iname']==filter_out]])  \n",
    "        \n",
    "    if len(train_part3_filtered['Metadata_pert_iname'].unique())==2:\n",
    "        filter_out=random.choice(train_part3_filtered['Metadata_pert_iname'].unique())\n",
    "        gene_seen=pd.concat([gene_seen,train_part3_filtered[train_part3_filtered['Metadata_pert_iname']==filter_out]])\n",
    "train_part_total_final=train_part_total.drop(index=list(gene_seen['Unnamed: 0'].index))\n",
    "train_part_total_final=train_part_total_final.drop_duplicates()\n",
    "df_test1=gene_seen\n",
    "df_test1=df_test1[df_test1['Metadata_Plate'].isin(lst)]\n",
    "# train\n",
    "df_train=pd.concat([train_part_total_final,dftrain2]) \n",
    "df_test2=df_proceed[df_proceed['Metadata_gene'].isin(test_lst)]\n",
    "df_test2=df_test2.drop_duplicates()\n",
    "df_test2=df_test2[df_test2['Metadata_experiment_type']=='Compound']\n",
    "df_test2=df_test2[df_test2['Metadata_Plate'].isin(lst)]\n",
    "# SEEN\n",
    "\n",
    "# Train_test pre-process\n",
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "df_test1_rm_emptygene=df_test1[df_test1['Metadata_gene']!='empty']\n",
    "y_test=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=df_test1_rm_emptygene[df_test1_rm_emptygene['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)\n",
    "df_proceed = df_proceed[df_proceed['Metadata_Plate'].isin(lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bfa4f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=130\n",
    "CP_seen_result= kNCM(trainX, trainY, testX, testY, k,'top_10')\n",
    "CP_seen_result['If_seen']='seen'\n",
    "CP_seen_result1= kNCM(trainX, trainY, testX, testY, k,'top_1')\n",
    "CP_seen_result1['If_seen']='seen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "36ac2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=df_train['Metadata_gene']\n",
    "trainX=df_train.drop('Metadata_gene', axis = 1)\n",
    "df_test2_rm_emptygene=df_test2[df_test2['Metadata_gene']!='empty']\n",
    "y_test=df_test2_rm_emptygene[df_test2_rm_emptygene['Metadata_experiment_type']=='Compound']['Metadata_gene']\n",
    "testX=df_test2_rm_emptygene[df_test2_rm_emptygene['Metadata_experiment_type']=='Compound'].drop('Metadata_gene', axis = 1)\n",
    "trainX=trainX.iloc[:,trainX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "testX=testX.iloc[:,testX.columns.get_loc(\"Cells_AreaShape_BoundingBoxMaximum_Y\"):]\n",
    "trainX=np.array(trainX)\n",
    "testX=np.array(testX)\n",
    "trainY=np.array(y_train)\n",
    "testY=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8b4b76cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP_KNCM_SEEN_TOP10 0.4624587458745874\n",
      "CP_KNCM_UNEEN_TOP10 0.08543217535153019\n",
      "CP_KNCM_TOTAL_TOP10 0.385272203877741\n",
      "\n",
      "CP_KNCM_SEEN_TOP1 0.12145214521452145\n",
      "CP_KNCM_UNEEN_TOP1 0.001282051282051282\n",
      "CP_KNCM_TOTAL_TOP1 0.09685039370078741\n"
     ]
    }
   ],
   "source": [
    "CP_unseen_result= kNCM(trainX, trainY, testX, testY, k,'top_10')\n",
    "CP_unseen_result['If_seen']='unseen'\n",
    "CP_total_result=pd.concat([CP_seen_result,CP_unseen_result])\n",
    "print('CP_KNCM_SEEN_TOP10',CP_seen_result['score'].mean())\n",
    "print('CP_KNCM_UNEEN_TOP10',CP_unseen_result['score'].mean())\n",
    "print('CP_KNCM_TOTAL_TOP10',CP_total_result['score'].mean())\n",
    "CP_unseen_result1= kNCM(trainX, trainY, testX, testY, k,'top_1')\n",
    "CP_unseen_result1['If_seen']='unseen'\n",
    "CP_total_result1=pd.concat([CP_seen_result1,CP_unseen_result1])\n",
    "print('')\n",
    "print('CP_KNCM_SEEN_TOP1',CP_seen_result1['score'].mean())\n",
    "print('CP_KNCM_UNEEN_TOP1',CP_unseen_result1['score'].mean())\n",
    "print('CP_KNCM_TOTAL_TOP1',CP_total_result1['score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce791d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
